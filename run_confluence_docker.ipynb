{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confluence Module Docker Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp\n",
    "mnt_dir = '/home/travis/data/france_2025/test/empty_mnt' # Downloaded using: gdown 1xRltFZ1gyP_nvwHMJW-rIgClzXx8CSLC\n",
    "script_ouput_directory = '/home/travis/repos/run-confluence-locally/run_scripts'\n",
    "\n",
    "repo_directory = '/home/travis/repos' # Contains the github repos of the modules you want to run, check documentation for branches\n",
    "\n",
    "index_range = '0-10'  # Adjust the range as needed\n",
    "\n",
    "module_to_run = 'neobam' # Chose what module to run using the below command dict, they are listed in order. eg: run expanded_setfinder first\n",
    "\n",
    "# This is a dictionary of all of the Confluence module run commands translated to singularity run commands.\n",
    "# You should not have to change anything here.\n",
    "command_dict = {\n",
    "    'expanded_setfinder': f'docker run -v {mnt_dir}/input:/data setfinder -r reaches_of_interest.json -c continent.json -e -s 16 -o /data -n /data -a MetroMan HiVDI SIC NeoBAM -i index_to_run',\n",
    "    'expanded_combine_data': f'docker run -v {mnt_dir}/input:/data combine_data -d /data -e -s 16',\n",
    "    'input': f'docker run -v {mnt_dir}/input:/mnt/data input -r /mnt/data/expanded_reaches_of_interest.json -i index_to_run',\n",
    "    'non_expanded_setfinder': f'docker run -v {mnt_dir}/input:/data setfinder -c continent.json -s 16 -o /data -n /data -a MetroMan HiVDI SIC NeoBAM -i index_to_run',\n",
    "    'non_expanded_combine_data': f'docker run -v {mnt_dir}/input:/data combine_data -d /data -s 16',\n",
    "    'prediagnostics': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/diagnostics/prediagnostics:/mnt/data/output prediagnostics -r reaches.json -i index_to_run',\n",
    "    'unconstrained_priors': f'docker run -v {mnt_dir}/input:/mnt/data priors -r unconstrained -p usgs riggs -g -s local -i index_to_run', # Branch local_run\n",
    "    'metroman': f'docker run --env AWS_BATCH_JOB_ID=\"foo\" -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/metroman:/mnt/data/output metroman -r metrosets.json -s local -v -i index_to_run', # branch local_run_args\n",
    "    'metroman_consolidation': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/metroman:/mnt/data/flpe metroman_consolidation -i index_to_run',\n",
    "    'unconstrained_momma': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/momma:/mnt/data/output momma -r reaches.json -m 3 -i index_to_run',\n",
    "    'neobam': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/geobam:/mnt/data/output neobam -r reaches.json -i index_to_run',\n",
    "    'sad': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/sad:/mnt/data/output sad --reachfile reaches.json --index index_to_run',\n",
    "    'moi': f'docker run --env AWS_BATCH_JOB_ID=\"foo\" -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/output moi -j basin.json -v -b unconstrained -s local -i index_to_run',\n",
    "    'unconstrained_offline': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/offline:/mnt/data/output offline unconstrained timeseries integrator reaches.json index_to_run',\n",
    "    'validation': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/offline:/mnt/data/offline -v {mnt_dir}/validation:/mnt/data/output validation reaches.json unconstrained index_to_run',\n",
    "    'output': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/diagnostics:/mnt/data/diagnostics -v {mnt_dir}/offline:/mnt/data/offline -v {mnt_dir}/validation:/mnt/data/validation -v {mnt_dir}/output:/mnt/data/output output -s local -j /app/metadata/metadata.json -m input priors prediagnostics momma neobam metroman sic4dvar sad moi offline validation swot -i index_to_run'\n",
    "}\n",
    "\n",
    "output_script_path = os.path.join(script_ouput_directory, f'run_{module_to_run}_{index_range}.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp\n",
    "\n",
    "def build_docker_image(repo_directory, module_to_run):\n",
    "    \"\"\"\n",
    "    Builds the Docker image for the specified module.\n",
    "\n",
    "    Parameters:\n",
    "    - repo_directory (str): Directory where the repos are located.\n",
    "    - module_to_run (str): The module to build the Docker image for.\n",
    "    \"\"\"\n",
    "    # Define the repository name (without expanded/non-expanded/etc.)\n",
    "    repo_name = module_to_run.replace('non_','').replace('expanded_', '').replace('non_expanded_', '').replace('unconstrained_', '').replace('constrained_', '')\n",
    "    repo_path = os.path.join(repo_directory, repo_name)\n",
    "    \n",
    "    # Build the Docker image\n",
    "    build_command = f'docker build -t {repo_name} {repo_path}'\n",
    "    print(f\"Building Docker image: {build_command}\")\n",
    "    sp.run(build_command, shell=True, check=True)\n",
    "\n",
    "def generate_run_script(command_dict, module_to_run, index_range, output_script_path, repo_directory, rebuild_docker):\n",
    "    \"\"\"\n",
    "    Generates a Python script that loops through a range of indices and runs the Docker command.\n",
    "    \n",
    "    Parameters:\n",
    "    - command_dict (dict): The dictionary of commands.\n",
    "    - module_to_run (str): The selected module to run.\n",
    "    - index_range (str): The range of indices (e.g., '0-100', '5-10', '7').\n",
    "    - output_script_path (str): The path to output the generated Python script.\n",
    "    - repo_directory (str): The path to the Confleunce repos\n",
    "    - rebuild_docker (bool): Rebuild the docker image based on a repo in the repo_directory\n",
    "    \"\"\"\n",
    "    # First, build the Docker image\n",
    "    if rebuild_docker:\n",
    "        build_docker_image(repo_directory, module_to_run)\n",
    "        \n",
    "    # Parse the index range\n",
    "    index_parts = index_range.split('-')\n",
    "    if len(index_parts) == 1:\n",
    "        start_index = int(index_parts[0])\n",
    "        end_index = int(index_parts[0])\n",
    "    else:\n",
    "        start_index = int(index_parts[0])\n",
    "        end_index = int(index_parts[1])\n",
    "    \n",
    "    # Create the Python script content with the loop\n",
    "    script_content = f\"\"\"\n",
    "import subprocess as sp\n",
    "\n",
    "# Docker command for the selected module\n",
    "command = f'{command_dict[module_to_run]}'\n",
    "\n",
    "# Loop through the specified index range and run the Docker command for each index\n",
    "for index in range({start_index}, {end_index} + 1):\n",
    "    print(f\"Running command for index {{index}}\")\n",
    "    run_command = command.replace('index_to_run', str(index))\n",
    "    sp.run(run_command, shell=True, check=True)\n",
    "\"\"\"\n",
    "    \n",
    "    # Write the generated script to the file\n",
    "    with open(output_script_path, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    print(f\"Python script created: {output_script_path}\")\n",
    "    return output_script_path\n",
    "\n",
    "\n",
    "\n",
    "# Then, generate the Python script that runs the command\n",
    "generate_run_script(command_dict = command_dict,\\\n",
    "                     module_to_run=module_to_run, \\\n",
    "                        index_range=index_range, \\\n",
    "                            output_script_path=output_script_path, \\\n",
    "                                repo_directory=repo_directory, \\\n",
    "                                    rebuild_docker = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running this notebook, there will be a {module name}.sh file generated in the same directory.\n",
    "# You can either add in an array and submit the job using sbatch or you can fill out the top of the cfl_wrapper.sh and have it submit jobs for you.\n",
    "# using the cfl_wrapper.sh is highly recommended if you are submitting a number of jobs larger than your HPC allows. I use it in all cases though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
