{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Confluence on an HPC\n",
    "\n",
    "# Requirements\n",
    "* docker installed somewhere where you have sudo priveledges to the point where \"docker --version\" completes successfully\n",
    "* singularity or apptainer installed on your HPC\n",
    "* a dockerhub account (free)\n",
    "\n",
    "\n",
    "# Overall Tasks\n",
    "* Git clone all of the repos you want to run to a machine where you have sudo priveledges and where \"docker --version\" works (locally)\n",
    "* Run the \"Prepare Images Locally\" section of this notebook locally\n",
    "* Run the \"Confluence Module SLURM Script Generator\" section of this notebook on your HPC to create SLURM submission scripts for each module\n",
    "* Run the Confluence Driver Script Generator section of this notebook on your HPC to create a SLURM submission script that runs each of the modules one by one (the one click run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Functions (IGNORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS IGNORE\n",
    "def build_and_push_images(repo_directory:str, target_repo_names:list, docker_username:str, push:bool = True, custom_tag_name:str = 'latest'):\n",
    "    for a_repo_name in target_repo_names:\n",
    "        repo_path = os.path.join(repo_directory, a_repo_name)\n",
    "        docker_path = f'{docker_username}/{a_repo_name}:{custom_tag_name}'\n",
    "        build_cmd = ['docker', 'build','--quiet', '-f', os.path.join(repo_path, \"Dockerfile\"), '-t', docker_path, repo_path]\n",
    "        try:\n",
    "            sp.run(build_cmd)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Docker build failed...\\n\"\n",
    "                f\"Build Command: {build_cmd}\\n\"\n",
    "                f\"Error: {e}\"\n",
    "            )\n",
    "        if push:\n",
    "            try:\n",
    "                push_cmd = ['docker', 'push','--quiet', docker_path]\n",
    "                sp.run(push_cmd)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Docker push failed...\\n\"\n",
    "                    f\"Push Command: {push_cmd}\\n\"\n",
    "                    f\"Error: {e}\"\n",
    "                )\n",
    "    \n",
    "def build_sifs_and_create_slurm_scripts(run_list, included_modules, base_dir, docker_username, build):\n",
    "\n",
    "    for run in run_list:\n",
    "        \n",
    "        # Has to exist with 'mnt' structure (Doit exister avec la structure 'mnt')\n",
    "        mnt_dir = os.path.join(base_dir, f'confluence_{run}', f'{run}_mnt')\n",
    "        \n",
    "        # Create the sh_scripts directory (Cree le repertoire sh_scripts)\n",
    "        sh_dir = os.path.join(base_dir, f'confluence_{run}', 'sh_scripts')\n",
    "        if not os.path.exists(sh_dir):\n",
    "            os.makedirs(sh_dir)\n",
    "        \n",
    "        # Create the sif directory (Cree la repertoire sif)\n",
    "        sif_dir = os.path.join(base_dir, f'confluence_{run}', 'sif')\n",
    "        if not os.path.exists(sif_dir):\n",
    "            os.makedirs(sif_dir)\n",
    "        \n",
    "        # Create the report directory (Cree la repertoire report)\n",
    "        report_dir = os.path.join(base_dir, f'confluence_{run}', 'report')\n",
    "        if not os.path.exists(report_dir):\n",
    "            os.makedirs(report_dir)\n",
    "\n",
    "\n",
    "        submission_prefix = '#SBATCH'\n",
    "\n",
    "\n",
    "        job_details = {\n",
    "        'partition': 'cpu-preempt',\n",
    "        'cpus-per-task': '1',\n",
    "        'job-name': f'{run}_cfl',\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "        command_dict = {\n",
    "            'expanded_setfinder': 'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'setfinder.simg') + ' -r reaches_of_interest.json -c continent.json -e -s 16 -o /data -n /data -a MetroMan HiVDI SIC NeoBAM -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'expanded_combine_data': 'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'combine_data.simg') + ' -d /data  -e -s 16',\n",
    "            'input': 'singularity run --bind ' + f'{mnt_dir}/input:/mnt/data ' + os.path.join(sif_dir, 'input.simg') + ' -r /mnt/data/expanded_reaches_of_interest.json -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'non_expanded_setfinder': 'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'setfinder.simg') + ' -c continent.json -s 16 -o /data -n /data -a MetroMan HiVDI SIC NeoBAM -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'non_expanded_combine_data': 'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'combine_data.simg') + ' -d /data -s 16',\n",
    "            'prediagnostics': 'singularity run --bind ' + f'{mnt_dir}/input:/mnt/data/input,{mnt_dir}/diagnostics/prediagnostics:/mnt/data/output ' + os.path.join(sif_dir, f'prediagnostics.simg') + ' -i ${SLURM_ARRAY_TASK_ID} -r reaches.json',\n",
    "            #'unconstrained_priors': f'singularity run -c --writable-tmpfs --bind {mnt_dir}/input:/mnt/data {os.path.join(sif_dir, \"priors.simg\")} ' + ' -i ${SLURM_ARRAY_TASK_ID} -r unconstrained -p usgs riggs -g -s local',\n",
    "            'hivdi': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/hivdi:/mnt/data/output ' + os.path.join(sif_dir, 'hivdi.simg') + ' /mnt/data/input/reaches.json --input-dir /mnt/data/input -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'sic4dvar': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/sic4dvar:/mnt/data/output,{mnt_dir}/logs:/mnt/data/logs '+ os.path.join(sif_dir, 'sic4dvar.simg') + ' -r reaches.json --index ${SLURM_ARRAY_TASK_ID}',\n",
    "            'metroman': f'singularity run --env AWS_BATCH_JOB_ID=”foo” --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/metroman:/mnt/data/output ' + os.path.join(sif_dir, \"metroman.simg\") + ' -i ${SLURM_ARRAY_TASK_ID} -r metrosets.json -s local -v',\n",
    "            'metroman_consolidation': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/metroman:/mnt/data/flpe ' + os.path.join(sif_dir, 'metroman_consolidation.simg') + ' -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'unconstrained_momma': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/momma:/mnt/data/output ' + os.path.join(sif_dir, 'momma.simg') + ' -r reaches.json -m 3 -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'neobam': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/geobam:/mnt/data/output ' + os.path.join(sif_dir, 'neobam.simg') + ' -r reaches.json -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'sad': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/sad:/mnt/data/output ' + os.path.join(sif_dir, 'sad.simg') + ' --reachfile reaches.json --index ${SLURM_ARRAY_TASK_ID}',\n",
    "            'MOI': f'singularity run --env AWS_BATCH_JOB_ID=\"foo\" --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/output ' + os.path.join(sif_dir, 'MOI.simg') + ' -j basin.json -v -b unconstrained -s local -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'unconstrained_offline': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/moi,{mnt_dir}/offline:/mnt/data/output ' + os.path.join(sif_dir, 'offline.simg') + ' unconstrained timeseries integrator reaches.json ${SLURM_ARRAY_TASK_ID}',\n",
    "            'Validation': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/moi, {mnt_dir}/offline:/mnt/data/offline,{mnt_dir}/validation:/mnt/data/output ' + os.path.join(sif_dir, 'Validation.simg') + ' reaches.json unconstrained ${SLURM_ARRAY_TASK_ID}',\n",
    "            'output': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/diagnostics:/mnt/data/diagnostics,{mnt_dir}/moi:/mnt/data/moi, {mnt_dir}/offline:/mnt/data/offline,{mnt_dir}/validation:/mnt/data/validation,{mnt_dir}/output:/mnt/data/output ' + os.path.join(sif_dir, 'output.simg') + ' -s local -j /app/metadata/metadata.json -m input priors prediagnostics momma hivdi neobam metroman sic4dvar sad validation swot -i ${SLURM_ARRAY_TASK_ID}'\n",
    "        }\n",
    "\n",
    "        def create_slurm_script(job_details=job_details, build_image=False, sif_dir='foo'):\n",
    "            submission_prefix = job_details['submission_prefix']\n",
    "            if build_image:\n",
    "                module_name = job_details['module_name']\n",
    "                image_name = module_name.replace('expanded_', '').replace('non_', '').replace('unconstrained_', '').replace('constrained_', '')\n",
    "                sp.run(['singularity', 'build', '-F', os.path.join(sif_dir, image_name + '.simg'), f\"docker://{job_details['docker_username']}/{image_name}\"])\n",
    "\n",
    "            file = open(os.path.join(sh_dir, f'{module_to_run}.sh'), 'w')\n",
    "            file.write('#!/bin/bash \\n')\n",
    "            file.write(f'{submission_prefix} -o {os.path.join(report_dir, f\"{module_to_run}.%a.out\")}' + ' \\n')\n",
    "\n",
    "            for item in job_details:\n",
    "                if item not in ['run_command', 'module_name', 'docker_username', 'submission_prefix']:\n",
    "                    file.write(f'{submission_prefix} --{item}={job_details[item]} \\n')\n",
    "            file.write(job_details[\"run_command\"])\n",
    "            file.close()\n",
    "\n",
    "        included_modules = included_modules\n",
    "\n",
    "        for module_to_run, run_command in command_dict.items():\n",
    "            \n",
    "            if module_to_run == 'MOI':\n",
    "                time_to_use = '00:30:00'\n",
    "                mem_to_use = '2G'\n",
    "            elif module_to_run == 'neobam':\n",
    "                time_to_use = '01:00:00'\n",
    "                mem_to_use = '6G'\n",
    "            elif module_to_run == 'output':\n",
    "                time_to_use = '05:00:00'\n",
    "                mem_to_use = '2G'\n",
    "            else:\n",
    "                time_to_use = '00:10:00'\n",
    "                mem_to_use = '2G'\n",
    "                \n",
    "            if included_modules:\n",
    "                if module_to_run not in included_modules:\n",
    "                    continue\n",
    "\n",
    "            print('DIRECTORY NAME: ', run, '\\nMODULE: ', module_to_run)\n",
    "            \n",
    "\n",
    "\n",
    "            job_details.update({\n",
    "                'run_command': run_command,\n",
    "                'module_name': module_to_run,\n",
    "                'mem': mem_to_use,\n",
    "                'time': time_to_use,\n",
    "                'docker_username': docker_username,\n",
    "                'submission_prefix': submission_prefix,\n",
    "                'job-name': f'{module_to_run}_{run}_cfl',\n",
    "\n",
    "            })\n",
    "            create_slurm_script(job_details=job_details, build_image=build, sif_dir=sif_dir)\n",
    "\n",
    "                \n",
    "def generate_slurm_driver(\n",
    "    job_name: str,\n",
    "    output_log_dir: str,\n",
    "    partition: str,\n",
    "    time_limit: str,\n",
    "    ntasks: int,\n",
    "    cpus_per_task: int,\n",
    "    mem: str,\n",
    "    total_jobs: int,\n",
    "    array_step: int,\n",
    "    batch_size: int,\n",
    "    concurrent_jobs: int,\n",
    "    directory: str,\n",
    "    scripts: list[str]\n",
    ") -> str:\n",
    "    slurm_header = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --output={output_log_dir}/{job_name}_%j.out\n",
    "#SBATCH --error={output_log_dir}/{job_name}_%j.err\n",
    "#SBATCH --partition={partition}\n",
    "#SBATCH --time={time_limit}\n",
    "#SBATCH --ntasks={ntasks}\n",
    "#SBATCH --cpus-per-task={cpus_per_task}\n",
    "#SBATCH --mem={mem}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Turn list of scripts into a bash array\n",
    "    script_array = '    ' + '\\n    '.join(scripts)\n",
    "    scripts_block = f\"\"\"scripts=(\n",
    "{script_array}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "    body = f\"\"\"# Load modules\n",
    "module load conda/latest\n",
    "\n",
    "# Job control variables\n",
    "total_jobs={total_jobs}\n",
    "array_step={array_step}\n",
    "batch_size={batch_size}\n",
    "concurrent_jobs={concurrent_jobs}\n",
    "directory=\"{directory}\"\n",
    "\n",
    "{scripts_block}\n",
    "\n",
    "for slurm_script in \"${{scripts[@]}}\"; do\n",
    "    echo \"Starting submission for: $slurm_script\"\n",
    "    date\n",
    "\n",
    "    # Override total_jobs based on script name\n",
    "    if [[ \"$slurm_script\" == *setfinder* ]]; then\n",
    "        current_total_jobs=7\n",
    "    elif [[ \"$slurm_script\" == *combine_data* ]]; then\n",
    "        current_total_jobs=1\n",
    "    elif [[ \"$slurm_script\" == *priors* ]]; then\n",
    "        current_total_jobs=7\n",
    "    elif [[ \"$slurm_script\" == *output* ]]; then\n",
    "        current_total_jobs=7\n",
    "    else\n",
    "        current_total_jobs=$total_jobs  # fallback to the default\n",
    "    fi\n",
    "\n",
    "    array_start=0\n",
    "    while [ $array_start -le $current_total_jobs ]; do\n",
    "        array_end=$((array_start + array_step - 1))\n",
    "        if [ $array_end -gt $current_total_jobs ]; then\n",
    "            array_end=$current_total_jobs\n",
    "        fi\n",
    "\n",
    "        echo \"Processing array range: $array_start-$array_end\"\n",
    "\n",
    "        start=$array_start\n",
    "        while [ $start -le $array_end ]; do\n",
    "            end=$((start + batch_size - 1))\n",
    "            if [ $end -gt $array_end ]; then\n",
    "                end=$array_end\n",
    "            fi\n",
    "\n",
    "            echo \"Submitting batch: $start-$end from $slurm_script\"\n",
    "            job_output=$(sbatch --array=${{start}}-${{end}}%${{concurrent_jobs}} \"${{directory}}/${{slurm_script}}\")\n",
    "            job_id=$(echo $job_output | awk '{{print $4}}')\n",
    "\n",
    "            echo \"Submitted batch $start-$end (Job ID: $job_id), waiting...\"\n",
    "            while squeue -j \"$job_id\" 2>/dev/null | grep -q \"$job_id\"; do\n",
    "                sleep 15\n",
    "            done\n",
    "\n",
    "            echo \"Finished batch $start-$end (Job ID: $job_id)\"\n",
    "            sleep 5\n",
    "\n",
    "            start=$((end + 1))\n",
    "        done\n",
    "\n",
    "        array_start=$((array_end + 1))\n",
    "    done\n",
    "done\n",
    "\n",
    "echo \"All submissions complete\"\n",
    "\"\"\"\n",
    "    return slurm_header + body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Prepare Docker Images (RUN LOCALLY, NOT HPC)\n",
    "* Builds docker images locally and stores them on your dockerhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------\n",
    "\n",
    "# SETUP\n",
    "\n",
    "# Directory where you are storing repos\n",
    "repo_directory = '/storage/repos'\n",
    "target_repo_names = ['setfinder', 'combine_data', 'input', 'momma']\n",
    "\n",
    "# Only provide this if you want to store images on dockerhub to move to HPC (you probably do)\n",
    "push = True\n",
    "docker_username = 'travissimmons'\n",
    "custom_tag_name = 'run1' # good to name same as the run\n",
    "\n",
    "# --------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_and_push_images(\\\n",
    "                      repo_directory = repo_directory, \\\n",
    "                      target_repo_names = target_repo_names, \\\n",
    "                      docker_username = docker_username, \\\n",
    "                      push = push, \\\n",
    "                      custom_tag_name = custom_tag_name \\\n",
    "                     )\n",
    "                      \n",
    "# The output should look something like \n",
    "# sha256:6900c3d99325a4a7c8b282d4a7a62f2a0f3fc673f03f5ca3333c2746bf20d06a\n",
    "# docker.io/travissimmons/setfinder:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Confluence Module SLURM Script Generator (RUN ON HPC, NOT LOCALLY)\n",
    "* Builds sif files from your dockerhub and generates scripts to submit to a SLURM job scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "\n",
    "# SETUP\n",
    "\n",
    "# Directory where you are storing repos\n",
    "base_dir = '/nas/cee-water/cjgleason/travis/data/offline_consolidation/confluence_runs/'\n",
    "included_modules= {'expanded_combine_data', 'expanded_setfinder', 'input', 'momma'}\n",
    "docker_username = 'travissimmons' # your dockerhub username here\n",
    "custom_tag_name = 'latest' # leave this as latest unless you have a really good reason!\n",
    "\n",
    "# Providing a run list will create slurm scripts to run, name your parent and _mnt folders with the same run* name\n",
    "run_list = ['run_1']\n",
    "\n",
    "# Rebuild the sif\n",
    "build = False\n",
    "\n",
    "# --------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORY NAME:  run_1 \n",
      "MODULE:  expanded_setfinder\n",
      "DIRECTORY NAME:  run_1 \n",
      "MODULE:  expanded_combine_data\n",
      "DIRECTORY NAME:  run_1 \n",
      "MODULE:  input\n",
      "DIRECTORY NAME:  run_1 \n",
      "MODULE:  neobam\n"
     ]
    }
   ],
   "source": [
    "build_sifs_and_create_slurm_scripts(run_list=run_list, \\\n",
    "                                    included_modules = included_modules, \\\n",
    "                                    base_dir = base_dir, \\\n",
    "                                    docker_username = docker_username,\n",
    "                                    build = build\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Confluence Driver Script Generator (RUN ON HPC, NOT LOCALLY)\n",
    "* Creates a batch submission script that will run all of your sif files in serial\n",
    "* use sbatch to submit the entire run\n",
    "* low resources and a long time should be used here, as all this job will do is launch your SLURM scripts you created for each module, it is basically a job manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = '/nas/cee-water/cjgleason/travis/data/offline_consolidation/confluence_runs/confluence_run_1'\n",
    "\n",
    "driver_script = generate_slurm_driver(\n",
    "    job_name=\"confluence_driver\",  # Sets the SLURM job name shown in the queue\n",
    "    output_log_dir=os.path.join(run_dir, \"driver_log\"),  # Directory to write SLURM output and error logs\n",
    "    partition=\"ceewater_cjgleason-cpu\",  # SLURM partition to submit the job to\n",
    "    time_limit=\"30:00:00\",  # Max time the whole workflow can run (HH:MM:SS)\n",
    "    ntasks=1,  # Number of tasks (Don't change)\n",
    "    cpus_per_task=1,  # Number of CPUs per task (Don't change)\n",
    "    mem=\"10G\",  # Memory required for driver (Don't change)\n",
    "    total_jobs=20,  # Total number of jobs (e.g., reach IDs) to process\n",
    "    array_step=10000,  # Size of each large array chunk (not used here since total_jobs < step)\n",
    "    batch_size=1000,  # Number of array tasks to submit in each batch\n",
    "    concurrent_jobs=400,  # Max number of jobs allowed to run at once within each batch\n",
    "    directory=os.path.join(run_dir, 'sh_scripts'),  # Path to job scripts\n",
    "    scripts=[\n",
    "        \"expanded_setfinder.sh\",\n",
    "        \"expanded_combine_data.sh\",\n",
    "        \"input.sh\",\n",
    "        \"momma.sh\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Write the generated SLURM driver script to a file for inspection or submission\n",
    "with open(os.path.join(run_dir, \"driver_submit_for_confluence_run_1.sh\"), \"w\") as f:\n",
    "    f.write(driver_script)\n",
    "\n",
    "# Optionally submit\n",
    "# import subprocess\n",
    "# subprocess.run([\"sbatch\", \"driver_submit.sh\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Running Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to run on specific reaches\n",
    "* modify the file at /mnt/input/reaches_of_interest.json\n",
    "#### In order to change a module and test it\n",
    "* change the module locally, build it and push to dockerhub using the first part of this notebook and then run as usual\n",
    "* you can use the run_list variable to generate more submission script per moule to test more than one change at a time. However, whenver you submit them, they will still run one at a time, it just submits the next run automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-swotEF]",
   "language": "python",
   "name": "conda-env-.conda-swotEF-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
