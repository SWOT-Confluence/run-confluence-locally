{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Confluence on an HPC\n",
    "\n",
    "# Requirements\n",
    "* docker installed somewhere where you have sudo priveledges to the point where \"docker --version\" completes successfully\n",
    "* singularity or apptainer installed on your HPC\n",
    "* a dockerhub account (free)\n",
    "\n",
    "\n",
    "# Overall Tasks\n",
    "* Pull all repos you want to run\n",
    "* Run the \"Prepare Images Locally\" section of this notebook where you have sudo priveledges and \"docker --version\" working\n",
    "* Run the \"Confluence Module SLURM Script Generator\" section of this notebook on your HPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Docker Images Locally\n",
    "* Builds docker images locally and stores them on your dockerhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------\n",
    "\n",
    "# SETUP\n",
    "\n",
    "# Directory where you are storing repos\n",
    "repo_directory = '/mnt/repos'\n",
    "target_repo_names = ['setfinder', 'input']\n",
    "\n",
    "# Only provide this if you want to store images on dockerhub to move to HPC (you probably do)\n",
    "push = True\n",
    "docker_username = 'tsimmons'\n",
    "custom_tag_name = 'latest' # leave this as latest unless you have a really good reason!\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def build_and_push_images(repo_directory:str, target_repo_names:list, docker_username:str, push:bool = True, custom_tag_name:str = 'latest'):\n",
    "    for a_repo_name in target_repo_names:\n",
    "        repo_path = os.path.join(repo_directory, a_repo_name)\n",
    "        docker_path = f'{docker_username}/{a_repo_name}:{custom_tag_name}'\n",
    "        build_cmd = ['docker', 'build', '-f', os.path.join(repo_path, \"Dockerfile\"), '-t', docker_path]\n",
    "        try:\n",
    "            sp.run(build_cmd)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Docker build failed...\\n\"\n",
    "                f\"Build Command: {build_cmd}\\n\"\n",
    "                f\"Error: {e}\"\n",
    "            )\n",
    "        if push:\n",
    "            try:\n",
    "                push_cmd = ['docker', 'push', docker_path]\n",
    "                sp.run(push_cmd)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Docker push failed...\\n\"\n",
    "                    f\"Push Command: {push_cmd}\\n\"\n",
    "                    f\"Error: {e}\"\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "('Docker build failed...', ['docker', 'build', '-f', '/mnt/repos/setfinder/Dockerfile', '-t', 'tsimmons/setfinder:latest'], '\\n', NameError(\"name 'sp' is not defined\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m, in \u001b[0;36mbuild_and_push_images\u001b[0;34m(repo_directory, target_repo_names, docker_username, push, custom_tag_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     sp\u001b[38;5;241m.\u001b[39mrun(build_cmd)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sp' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m build_and_push_images(\\\n\u001b[1;32m      2\u001b[0m                       repo_directory \u001b[38;5;241m=\u001b[39m repo_directory, \\\n\u001b[1;32m      3\u001b[0m                       target_repo_names \u001b[38;5;241m=\u001b[39m target_repo_names, \\\n\u001b[1;32m      4\u001b[0m                       docker_username \u001b[38;5;241m=\u001b[39m docker_username, \\\n\u001b[1;32m      5\u001b[0m                       push \u001b[38;5;241m=\u001b[39m push, \\\n\u001b[1;32m      6\u001b[0m                       custom_tag_name \u001b[38;5;241m=\u001b[39m custom_tag_name \\\n\u001b[1;32m      7\u001b[0m                      )\n",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m, in \u001b[0;36mbuild_and_push_images\u001b[0;34m(repo_directory, target_repo_names, docker_username, push, custom_tag_name)\u001b[0m\n\u001b[1;32m     18\u001b[0m     sp\u001b[38;5;241m.\u001b[39mrun(build_cmd)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDocker build failed...\u001b[39m\u001b[38;5;124m'\u001b[39m, build_cmd,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, e)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m push:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ('Docker build failed...', ['docker', 'build', '-f', '/mnt/repos/setfinder/Dockerfile', '-t', 'tsimmons/setfinder:latest'], '\\n', NameError(\"name 'sp' is not defined\"))"
     ]
    }
   ],
   "source": [
    "build_and_push_images(\\\n",
    "                      repo_directory = repo_directory, \\\n",
    "                      target_repo_names = target_repo_names, \\\n",
    "                      docker_username = docker_username, \\\n",
    "                      push = push, \\\n",
    "                      custom_tag_name = custom_tag_name \\\n",
    "                     )\n",
    "                      \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Docker build failed...\nBuild Command: ['docker', 'build', '-f', '/mnt/repos/setfinder/Dockerfile', '-t', 'tsimmons/setfinder:latest']\nError: [Errno 2] No such file or directory: 'docker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 22\u001b[0m, in \u001b[0;36mbuild_and_push_images\u001b[0;34m(repo_directory, target_repo_names, docker_username, push, custom_tag_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     sp\u001b[38;5;241m.\u001b[39mrun(build_cmd)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/work/pi_cjgleason_umass_edu/.conda/envs/swotEF/lib/python3.12/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/work/pi_cjgleason_umass_edu/.conda/envs/swotEF/lib/python3.12/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[1;32m   1032\u001b[0m                         restore_signals,\n\u001b[1;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/work/pi_cjgleason_umass_edu/.conda/envs/swotEF/lib/python3.12/subprocess.py:1953\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'docker'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m build_and_push_images(\\\n\u001b[1;32m      2\u001b[0m                       repo_directory \u001b[38;5;241m=\u001b[39m repo_directory, \\\n\u001b[1;32m      3\u001b[0m                       target_repo_names \u001b[38;5;241m=\u001b[39m target_repo_names, \\\n\u001b[1;32m      4\u001b[0m                       docker_username \u001b[38;5;241m=\u001b[39m docker_username, \\\n\u001b[1;32m      5\u001b[0m                       push \u001b[38;5;241m=\u001b[39m push, \\\n\u001b[1;32m      6\u001b[0m                       custom_tag_name \u001b[38;5;241m=\u001b[39m custom_tag_name \\\n\u001b[1;32m      7\u001b[0m                      )\n",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m, in \u001b[0;36mbuild_and_push_images\u001b[0;34m(repo_directory, target_repo_names, docker_username, push, custom_tag_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m     sp\u001b[38;5;241m.\u001b[39mrun(build_cmd)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocker build failed...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild Command: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbuild_cmd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m push:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Docker build failed...\nBuild Command: ['docker', 'build', '-f', '/mnt/repos/setfinder/Dockerfile', '-t', 'tsimmons/setfinder:latest']\nError: [Errno 2] No such file or directory: 'docker'"
     ]
    }
   ],
   "source": [
    "build_and_push_images(\\\n",
    "                      repo_directory = repo_directory, \\\n",
    "                      target_repo_names = target_repo_names, \\\n",
    "                      docker_username = docker_username, \\\n",
    "                      push = push, \\\n",
    "                      custom_tag_name = custom_tag_name \\\n",
    "                     )\n",
    "                      \n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confluence Module SLURM Script Generator\n",
    "\n",
    "### Confluence Module SLURM Script Generator\n",
    "* Builds sif files from your dockerhub and generates scripts to submit to a SLURM job scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_runs(run_list, included_modules, excluded_modules, base_dir, docker_username):\n",
    "    import os\n",
    "    import subprocess as sp\n",
    "\n",
    "    for run in run_list:\n",
    "        \n",
    "        # Has to exist with 'mnt' structure (Doit exister avec la structure 'mnt')\n",
    "        mnt_dir = os.path.join(base_dir, f'confluence_{run}', f'{run}_mnt')\n",
    "        \n",
    "        # Create the sh_scripts directory (Cree le repertoire sh_scripts)\n",
    "        sh_dir = os.path.join(base_dir, f'confluence_{run}', 'sh_scripts')\n",
    "        if not os.path.exists(sh_dir):\n",
    "            os.mkdir(sh_dir)\n",
    "        \n",
    "        # Create the sif directory (Cree la repertoire sif)\n",
    "        sif_dir = os.path.join(base_dir, f'confluence_{run}', 'sif')\n",
    "        if not os.path.exists(sif_dir):\n",
    "            os.mkdir(sif_dir)\n",
    "        \n",
    "        # Create the report directory (Cree la repertoire report)\n",
    "        report_dir = os.path.join(base_dir, f'confluence_{run}', 'report')\n",
    "        if not os.path.exists(report_dir):\n",
    "            os.mkdir(report_dir)\n",
    "\n",
    "\n",
    "        submission_prefix = '#SBATCH'\n",
    "\n",
    "        job_details = {\n",
    "            'partition': 'cpu-preempt',\n",
    "            'cpus-per-task': '1',\n",
    "            'mem': '2G',\n",
    "            'time': '00:10:00',\n",
    "            'job-name': f'{run}_cfl',\n",
    "        }\n",
    "\n",
    "        command_dict = {\n",
    "            'expanded_setfinder': 'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'setfinder.simg') + ' -r reaches_of_interest.json -c continent.json -e -s 16 -o /data -n /data -a MetroMan HiVDI SIC NeoBAM -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'expanded_combine_data': 'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'combine_data.simg') + ' -d /data  -e -s 16',\n",
    "            'input_fs': 'singularity run --bind ' + f'{mnt_dir}/input:/mnt/data ' + os.path.join(sif_dir, 'input_fs.simg') + ' -r /mnt/data/expanded_reaches_of_interest.json -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'non_expanded_setfinder': 'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'setfinder.simg') + ' -c continent.json -s 16 -o /data -n /data -a MetroMan HiVDI SIC NeoBAM -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'non_expanded_combine_data': 'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'combine_data.simg') + ' -d /data -s 16',\n",
    "            'prediagnostics_s_bb_bb': 'singularity run --bind ' + f'{mnt_dir}/input:/mnt/data/input,{mnt_dir}/diagnostics/prediagnostics:/mnt/data/output ' + os.path.join(sif_dir, f'prediagnostics_s_bb_bb.simg') + ' -i ${SLURM_ARRAY_TASK_ID} -r reaches.json',\n",
    "            'prediagnostics_strict': 'singularity run --bind ' + f'{mnt_dir}/input:/mnt/data/input,{mnt_dir}/diagnostics/prediagnostics:/mnt/data/output ' + os.path.join(sif_dir, 'prediagnostics_strict.simg') + ' -i ${SLURM_ARRAY_TASK_ID} -r reaches.json',\n",
    "            'prediagnostics_permissive': 'singularity run --bind ' + f'{mnt_dir}/input:/mnt/data/input,{mnt_dir}/diagnostics/prediagnostics:/mnt/data/output ' + os.path.join(sif_dir, 'prediagnostics_permissive.simg') + ' -i ${SLURM_ARRAY_TASK_ID} -r reaches.json',\n",
    "            'prediagnostics_relaxed': 'singularity run --bind ' + f'{mnt_dir}/input:/mnt/data/input,{mnt_dir}/diagnostics/prediagnostics:/mnt/data/output ' + os.path.join(sif_dir, 'prediagnostics_relaxed.simg') + ' -i ${SLURM_ARRAY_TASK_ID} -r reaches.json',\n",
    "            #'unconstrained_priors': f'singularity run -c --writable-tmpfs --bind {mnt_dir}/input:/mnt/data {os.path.join(sif_dir, \"priors.simg\")} ' + ' -i ${SLURM_ARRAY_TASK_ID} -r unconstrained -p usgs riggs -g -s local',\n",
    "            'hivdi': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/hivdi:/mnt/data/output ' + os.path.join(sif_dir, 'hivdi.simg') + ' /mnt/data/input/reaches.json --input-dir /mnt/data/input -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'sic4dvar': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/sic4dvar:/mnt/data/output,{mnt_dir}/logs:/mnt/data/logs '+ os.path.join(sif_dir, 'sic4dvar.simg') + ' -r reaches.json --index ${SLURM_ARRAY_TASK_ID}',\n",
    "            'metroman': f'singularity run --env AWS_BATCH_JOB_ID=”foo” --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/metroman:/mnt/data/output ' + os.path.join(sif_dir, \"metroman.simg\") + ' -i ${SLURM_ARRAY_TASK_ID} -r metrosets.json -s local -v',\n",
    "            'metroman_consolidation': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/metroman:/mnt/data/flpe ' + os.path.join(sif_dir, 'metroman_consolidation.simg') + ' -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'unconstrained_momma': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/momma:/mnt/data/output ' + os.path.join(sif_dir, 'momma.simg') + ' -r reaches.json -m 3 -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'neobam': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/geobam:/mnt/data/output ' + os.path.join(sif_dir, 'neobam.simg') + ' -r reaches.json -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'sad': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/sad:/mnt/data/output ' + os.path.join(sif_dir, 'sad.simg') + ' --reachfile reaches.json --index ${SLURM_ARRAY_TASK_ID}',\n",
    "            'moi': f'singularity run --env AWS_BATCH_JOB_ID=\"foo\" --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/output ' + os.path.join(sif_dir, 'moi.simg') + ' -j basin.json -v -b unconstrained -s local -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'unconstrained_offline': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/moi,{mnt_dir}/offline:/mnt/data/output ' + os.path.join(sif_dir, 'offline.simg') + ' unconstrained timeseries integrator reaches.json ${SLURM_ARRAY_TASK_ID}',\n",
    "            'validation': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/moi, {mnt_dir}/offline:/mnt/data/offline,{mnt_dir}/validation:/mnt/data/output ' + os.path.join(sif_dir, 'validation.simg') + ' reaches.json unconstrained ${SLURM_ARRAY_TASK_ID}',\n",
    "            'output': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/diagnostics:/mnt/data/diagnostics,{mnt_dir}/moi:/mnt/data/moi, {mnt_dir}/offline:/mnt/data/offline,{mnt_dir}/validation:/mnt/data/validation,{mnt_dir}/output:/mnt/data/output ' + os.path.join(sif_dir, 'output.simg') + ' -s local -j /app/metadata/metadata.json -m input priors prediagnostics momma hivdi neobam metroman sic4dvar sad validation swot -i ${SLURM_ARRAY_TASK_ID}'\n",
    "        }\n",
    "\n",
    "        def create_slurm_script(job_details=job_details, build_image=False, sif_dir='foo'):\n",
    "            submission_prefix = job_details['submission_prefix']\n",
    "            if build_image:\n",
    "                module_name = job_details['module_name']\n",
    "                image_name = module_name.replace('expanded_', '').replace('non_', '').replace('unconstrained_', '').replace('constrained_', '')\n",
    "                sp.run(['singularity', 'build', '-F', os.path.join(sif_dir, image_name + '.simg'), f\"docker://{job_details['docker_username']}/{image_name}\"])\n",
    "\n",
    "            file = open(os.path.join(sh_dir, f'{module_to_run}.sh'), 'w')\n",
    "            file.write('#!/bin/bash \\n')\n",
    "            file.write(f'{submission_prefix} -o {os.path.join(report_dir, f\"{module_to_run}.%a.out\")}' + ' \\n')\n",
    "\n",
    "            for item in job_details:\n",
    "                if item not in ['run_command', 'module_name', 'docker_username', 'submission_prefix']:\n",
    "                    file.write(f'{submission_prefix} --{item}={job_details[item]} \\n')\n",
    "            file.write(job_details[\"run_command\"])\n",
    "            file.close()\n",
    "\n",
    "        excluded_modules = excluded_modules\n",
    "        included_modules = included_modules\n",
    "\n",
    "        for module_to_run, run_command in command_dict.items():\n",
    "            if excluded_modules:\n",
    "                if module_to_run in excluded_modules:\n",
    "                    continue\n",
    "            elif included_modules:\n",
    "                if module_to_run not in included_modules:\n",
    "                    continue\n",
    "\n",
    "            print('DIRECTORY NAME: ', run, '\\nMODULE: ', module_to_run)\n",
    "\n",
    "            if module_to_run == 'hivdi':\n",
    "                docker_user_to_use = 'travissimmons'\n",
    "            else:\n",
    "                docker_user_to_use = docker_username\n",
    "\n",
    "            job_details.update({\n",
    "                'run_command': run_command,\n",
    "                'module_name': module_to_run,\n",
    "                'docker_username': docker_user_to_use,\n",
    "                'submission_prefix': submission_prefix,\n",
    "                'job-name': f'{module_to_run}_{run}_cfl',\n",
    "\n",
    "            })\n",
    "            create_slurm_script(job_details=job_details, build_image=True, sif_dir=sif_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_runs(run_list=['fs_s'], \\\n",
    "             included_modules= {'expanded_setfinder','expanded_combine_data', \\\n",
    "                                'input_fs', 'non_expanded_setfinder', 'non_expanded_combine_data', \\\n",
    "                                'prediagnostics_s_bb_bb', 'hivdi', 'sic4dvar', 'metroman', 'metroman_consolidation', \\\n",
    "                                'unconstrained_momma', 'neobam'}, \\\n",
    "             excluded_modules={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running this notebook, there will be a {module name}.sh file generated in the same directory.\n",
    "# You can either add in an array and submit the job using sbatch or you can fill out the top of the cfl_wrapper.sh and have it submit jobs for you.\n",
    "# using the cfl_wrapper.sh is highly recommended if you are submitting a number of jobs larger than your HPC allows. I use it in all cases though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit Reaches of Interest for Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # EF\n",
    "# # # Edit reaches of interest as needed\n",
    "\n",
    "# import json\n",
    "\n",
    "# json_filename = 'reaches_of_interest_fs.json'\n",
    "# x_elements = 10000\n",
    "# # Load the JSON file\n",
    "# with open(f\"/nas/cee-water/cjgleason/ellie/SWOT/confluence/confluence_fs_s/fs_s_mnt/input/{json_filename}\", \"r\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # Ensure data is a list before slicing\n",
    "# if isinstance(data, list):\n",
    "#     data = data[x_elements:] # Keep last x elements OR data = data[:x_elements]  # Keep only the first x elements\n",
    "#     data = [str(x) for x in data] # Make sure they are in the correct data type\n",
    "# # Save the modified JSON\n",
    "# with open(\"/nas/cee-water/cjgleason/ellie/SWOT/confluence/confluence_fs_s1/fs_s1_mnt/input/reaches_of_interest.json\", \"w\") as f:\n",
    "#     json.dump(data, f, indent=4)\n",
    "\n",
    "# print(f\"Saved {x_elements} of {json_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-swotEF]",
   "language": "python",
   "name": "conda-env-.conda-swotEF-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
