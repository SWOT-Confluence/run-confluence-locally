{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confluence Module Docker Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Confluence on a LOCAL Machine\n",
    "\n",
    "## Requirements\n",
    "* docker installed somewhere where you have sudo priveledges to the point where \"docker --version\" completes successfully\n",
    "* a python environment\n",
    "\n",
    "## Optional\n",
    "* a dockerhub account (free, good for initial transition to HPC, sharing, versioning)\n",
    "\n",
    "\n",
    "## Run Confluence \n",
    "1. Git clone all of the repos you want to run to a machine where you have sudo priveledges and where \"docker --version\" works (locally)\n",
    "2. Prep an empty_mnt directory to store confluence run (requires gdown package in environment)\n",
    "3. Run the \"build_and_push_images\" function of this notebook locally\n",
    "4. Run the \"generate_run_scripts\" section of this notebook to create python submission scripts for each module\n",
    "5. Run the generate_all_modules_bash Confluence Driver Script Generator section of this notebook  to create a .sh submission script that runs each of the modules one by one (the one click run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp\n",
    "\n",
    "# FUNCTIONS IGNORE\n",
    "\n",
    "def clone_repos(repo_dir, repo_names, name_map, branch='main'):\n",
    "    \"\"\"Clone repositories with specified branch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    repo_dir : str\n",
    "        Directory to clone repos into\n",
    "    repo_names : list\n",
    "        List of repository names to clone\n",
    "    branch : str or dict, optional\n",
    "        Branch name to clone. Can be:\n",
    "        - A string: same branch for all repos (default: 'main')\n",
    "        - A dict: mapping repo name to specific branch\n",
    "    \"\"\"\n",
    "    os.makedirs(repo_dir, exist_ok=True)\n",
    "    \n",
    "    for name in repo_names:\n",
    "        path = os.path.join(repo_dir, name)\n",
    "        repo_name = name_map.get(name, name)\n",
    "        url = f'https://github.com/SWOT-Confluence/{repo_name}.git'\n",
    "        \n",
    "        # Determine which branch to use\n",
    "        if isinstance(branch, dict):\n",
    "            branch_name = branch.get(name, 'main')\n",
    "        else:\n",
    "            branch_name = branch\n",
    "        \n",
    "        if os.path.exists(path):\n",
    "            print(f'[Remove] Deleting existing {name} to overwrite...')\n",
    "            try:\n",
    "                shutil.rmtree(path)  # rm -rf\n",
    "            except OSError as e:\n",
    "                print(f\"Error: {path} : {e.strerror}\")\n",
    "        \n",
    "        print(f'[Clone] Cloning {name} from branch {branch_name}...')\n",
    "        sp.run(['git', 'clone', '--branch', branch_name, url, name], cwd=repo_dir)\n",
    "\n",
    "\n",
    "\n",
    "def build_and_push_images(repo_directory:str, modules_to_run:list, docker_username:str, push:bool = True, custom_tag_name:str = 'latest'):\n",
    "    for a_repo_name in modules_to_run:\n",
    "        repo_path = os.path.join(repo_directory, a_repo_name)\n",
    "        docker_path = f'{docker_username}/{a_repo_name}:{custom_tag_name}'\n",
    "        build_cmd = ['docker', 'build','--quiet', '-f', os.path.join(repo_path, \"Dockerfile\"), '-t', docker_path, repo_path]\n",
    "        try:\n",
    "            sp.run(build_cmd)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Docker build failed...\\n\"\n",
    "                f\"Build Command: {build_cmd}\\n\"\n",
    "                f\"Error: {e}\"\n",
    "            )\n",
    "        if push:\n",
    "            try:\n",
    "                push_cmd = ['docker', 'push','--quiet', docker_path]\n",
    "                sp.run(push_cmd)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Docker push failed...\\n\"\n",
    "                    f\"Push Command: {push_cmd}\\n\"\n",
    "                    f\"Error: {e}\"\n",
    "                )\n",
    "\n",
    "import os\n",
    "import json\n",
    "import subprocess as sp\n",
    "\n",
    "def generate_run_scripts(\n",
    "    run,\n",
    "    modules_to_run,          # list[str]\n",
    "    script_jobs,             # dict of module -> job count (str), e.g., \"7\" or \"$default_jobs\"\n",
    "    base_dir,\n",
    "    repo_directory,\n",
    "    rebuild_docker,\n",
    "    docker_username,\n",
    "    push,\n",
    "    custom_tag_name,\n",
    "    default_jobs_json_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a separate Python run script for each module in modules_to_run.\n",
    "    If a module's job count is \"$default_jobs\", dynamically determine the number\n",
    "    of jobs from the length of the JSON list at default_jobs_json_path.\n",
    "\n",
    "    Parameters:\n",
    "    - run (str): Name of the confluence run\n",
    "    - modules_to_run (list[str]): Modules to generate scripts for.\n",
    "    - script_jobs (dict): Module -> job count (int as str or \"$default_jobs\").\n",
    "    - base_dir (str): Path to repos.\n",
    "    - rebuild_docker (bool): Whether to rebuild docker images.\n",
    "    - default_jobs_json_path (str): JSON file path to determine default_jobs length.\n",
    "    \"\"\"\n",
    "    # Has to exist with 'mnt' structure (Doit exister avec la structure 'mnt')\n",
    "    mnt_dir = os.path.join(base_dir, f'confluence_{run}', f'{run}_mnt')\n",
    "    \n",
    "    # Create the sh_scripts directory (Cree le repertoire sh_scripts)\n",
    "    sh_dir = os.path.join(base_dir, f'confluence_{run}', 'sh_scripts')\n",
    "    if not os.path.exists(sh_dir):\n",
    "        os.makedirs(sh_dir)\n",
    "    \n",
    "    # Create the sif directory (Cree la repertoire sif)\n",
    "    sif_dir = os.path.join(base_dir, f'confluence_{run}', 'sif')\n",
    "    if not os.path.exists(sif_dir):\n",
    "        os.makedirs(sif_dir)\n",
    "    \n",
    "    # Create the report directory (Cree la repertoire report)\n",
    "    report_dir = os.path.join(base_dir, f'confluence_{run}', 'report')\n",
    "    if not os.path.exists(report_dir):\n",
    "        os.makedirs(report_dir)\n",
    "\n",
    "    # This is a dictionary of all of the Confluence module run commands translated to singularity run commands.\n",
    "    # You should not have to change anything here.\n",
    "    command_dict = {\n",
    "        'expanded_setfinder': f'docker run -v {mnt_dir}/input:/data setfinder -r reaches_of_interest.json -c continent.json -e -s 17 -o /data -n /data -a MetroMan HiVDI SIC -i index_to_run',\n",
    "        'expanded_combine_data': f'docker run -v {mnt_dir}/input:/data combine_data -d /data -e -s 17',\n",
    "        'input': f'docker run -v {mnt_dir}/input:/mnt/data input -v 17 -r /mnt/data/expanded_reaches_of_interest.json -c SWOT_L2_HR_RiverSP_D -i index_to_run',\n",
    "        'non_expanded_setfinder': f'docker run -v {mnt_dir}/input:/data setfinder -c continent.json -s 17 -o /data -n /data -a MetroMan HiVDI SIC -i index_to_run',\n",
    "        'non_expanded_combine_data': f'docker run -v {mnt_dir}/input:/data combine_data -d /data -s 17',\n",
    "        'prediagnostics': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/diagnostics/prediagnostics:/mnt/data/output prediagnostics -r reaches.json -i index_to_run',\n",
    "        'unconstrained_priors': f'docker run -v {mnt_dir}/input:/mnt/data priors -r unconstrained -p usgs riggs -g -s local -i index_to_run', \n",
    "        'constrained_priors': f'docker run -v {mnt_dir}/input:/mnt/data priors -r constrained -p usgs riggs -g -s local -i index_to_run', \n",
    "        'metroman': f'docker run --env AWS_BATCH_JOB_ID=\"foo\" -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/metroman:/mnt/data/output metroman -r metrosets.json -s local -v -i index_to_run',\n",
    "        'metroman_consolidation': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/metroman:/mnt/data/flpe metroman_consolidation -i index_to_run',\n",
    "        'unconstrained_momma': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/momma:/mnt/data/output momma -r reaches.json -m 3 -i index_to_run',\n",
    "        'neobam': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/geobam:/mnt/data/output neobam -r reaches.json -i index_to_run',\n",
    "        'sad': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/sad:/mnt/data/output sad --reachfile reaches.json --index index_to_run',\n",
    "        'moi': f'docker run --env AWS_BATCH_JOB_ID=\"foo\" -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/output moi -j basin.json -v -b unconstrained -i index_to_run', #-s local\n",
    "        'consensus': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe --mntdir /mnt/data -r /mnt/data/input/reaches.json -i index_to_run',\n",
    "        'unconstrained_offline': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/offline:/mnt/data/output offline unconstrained timeseries integrator reaches.json index_to_run',\n",
    "        'validation': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/offline:/mnt/data/offline -v {mnt_dir}/validation:/mnt/data/output validation reaches.json unconstrained index_to_run',\n",
    "        # 'output': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/diagnostics:/mnt/data/diagnostics -v {mnt_dir}/offline:/mnt/data/offline -v {mnt_dir}/validation:/mnt/data/validation -v {mnt_dir}/output:/mnt/data/output output -s local -j /app/metadata/metadata.json -m input prediagnostics momma hivdi metroman sic4dvar sad moi consensus offline validation swot priors -v 17 -i index_to_run'\n",
    "        'output': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/diagnostics:/mnt/data/diagnostics -v {mnt_dir}/offline:/mnt/data/offline -v {mnt_dir}/validation:/mnt/data/validation -v {mnt_dir}/output:/mnt/data/output output -s local -j /app/metadata/metadata.json -m input prediagnostics momma metroman sic4dvar sad consensus swot priors -v 17 -i index_to_run'\n",
    "\n",
    "    }\n",
    "    \n",
    "    # Build docker images once if requested\n",
    "    if rebuild_docker:\n",
    "        build_and_push_images(\n",
    "            repo_directory=repo_directory,\n",
    "            modules_to_run=modules_to_run,\n",
    "            docker_username=docker_username,\n",
    "            push=push,\n",
    "            custom_tag_name=custom_tag_name\n",
    "        )\n",
    "\n",
    "    # Load default_jobs from JSON if needed\n",
    "    default_jobs = None\n",
    "    if \"$default_jobs\" in script_jobs.values():\n",
    "        if default_jobs_json_path is None:\n",
    "            raise ValueError(\"default_jobs_json_path must be provided when using $default_jobs\")\n",
    "        with open(default_jobs_json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                default_jobs = len(data)\n",
    "            else:\n",
    "                raise ValueError(f\"Expected a JSON list at {default_jobs_json_path}\")\n",
    "\n",
    "    output_paths = []\n",
    "\n",
    "    for module in modules_to_run:\n",
    "        job_count = script_jobs.get(module, \"1\")\n",
    "        if job_count == \"$default_jobs\":\n",
    "            if default_jobs is None:\n",
    "                raise ValueError(f\"default_jobs JSON not loaded for module {module}\")\n",
    "            start_index = 0\n",
    "            end_index = default_jobs - 1\n",
    "        else:\n",
    "            start_index = 0\n",
    "            end_index = int(job_count) - 1\n",
    "\n",
    "        script_content = f\"\"\"#!/usr/bin/env python3\n",
    "import subprocess as sp\n",
    "\n",
    "# Docker command for module: {module}\n",
    "command = f'{command_dict[module]}'\n",
    "\n",
    "for index in range({start_index}, {end_index} + 1):\n",
    "    print(f\"Running command for module '{module}' at index {{index}}\")\n",
    "    run_command = command.replace('index_to_run', str(index))\n",
    "    sp.run(run_command, shell=True, check=True)\n",
    "\"\"\"\n",
    "\n",
    "        output_script_path = os.path.join(sh_dir, f\"run_{module}.py\")\n",
    "        with open(output_script_path, 'w') as f:\n",
    "            f.write(script_content)\n",
    "\n",
    "        os.chmod(output_script_path, 0o755)\n",
    "        output_paths.append(output_script_path)\n",
    "        print(f\"Python script created: {output_script_path}\")\n",
    "\n",
    "    return output_paths\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def generate_run_all_modules_bash(\n",
    "    modules_to_run: list[str],\n",
    "    script_jobs: dict,\n",
    "    base_dir: str,\n",
    "    script_folder_name: str = \"sh_scripts\",\n",
    "    bash_script_name: str = \"run_all_modules.sh\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates a Bash script that runs all per-module scripts in series.\n",
    "    Only includes modules for which a script was actually generated.\n",
    "\n",
    "    Parameters:\n",
    "    - modules_to_run: list of module names\n",
    "    - script_jobs: dict of module -> job count (str), used to filter modules\n",
    "    - base_dir: directory to save the bash script\n",
    "    - script_folder_name: folder inside base_dir where run_<module>.py scripts reside\n",
    "    - bash_script_name: name of the generated bash script\n",
    "\n",
    "    Returns:\n",
    "    - full path to the generated bash script\n",
    "    \"\"\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    bash_script_path = os.path.join(base_dir, f'confluence_{run}', bash_script_name)\n",
    "\n",
    "    # Only include modules that have non-zero job counts\n",
    "    filtered_modules = []\n",
    "    for module in modules_to_run:\n",
    "        count = script_jobs.get(module, \"0\")\n",
    "        if count != \"0\":\n",
    "            filtered_modules.append(module)\n",
    "\n",
    "    # Generate modules array string\n",
    "    modules_array_str = \"modules_to_run=(\\n\"\n",
    "    for module in filtered_modules:\n",
    "        modules_array_str += f\"    \\\"{module}\\\"\\n\"\n",
    "    modules_array_str += \")\\n\"\n",
    "\n",
    "    script_content = f\"\"\"#!/bin/bash\n",
    "# {bash_script_name}\n",
    "# Runs all generated module scripts in series\n",
    "\n",
    "SCRIPT_DIR=\"{script_folder_name}\"\n",
    "\n",
    "{modules_array_str}\n",
    "\n",
    "echo \"Starting module runs...\"\n",
    "\n",
    "for module in \"${{modules_to_run[@]}}\"; do\n",
    "    script_path=\"${{SCRIPT_DIR}}/run_${{module}}.py\"\n",
    "\n",
    "    if [[ -f \"$script_path\" ]]; then\n",
    "        echo \"---------------------------------------------\"\n",
    "        echo \"Running module: $module\"\n",
    "        echo \"Script: $script_path\"\n",
    "        echo \"---------------------------------------------\"\n",
    "        python3 \"$script_path\"\n",
    "        if [[ $? -ne 0 ]]; then\n",
    "            echo \"Error occurred while running $module. Exiting.\"\n",
    "            exit 1\n",
    "        fi\n",
    "        echo \"Finished module: $module\"\n",
    "        echo\n",
    "    else\n",
    "        echo \"Script not found for module: $module. Skipping.\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"All modules completed successfully.\"\n",
    "\"\"\"\n",
    "\n",
    "    with open(bash_script_path, \"w\") as f:\n",
    "        f.write(script_content)\n",
    "\n",
    "    os.chmod(bash_script_path, 0o755)\n",
    "    print(f\"Generated bash script: {bash_script_path}\")\n",
    "    return bash_script_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_NAME: Dtest\n",
      "REPO_DIR: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\modules/D\n",
      "SIF_DIR: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess as sp\n",
    "from pathlib import Path\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = r'C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence' #directory storing confluence runs\n",
    "REPO_DIR = os.path.join(BASE_DIR, 'modules/D') #directory storing repos i.e ./modules/\n",
    "RUN_NAME = 'Dtest' #Specific run name i.e. 'test'\n",
    "os.chdir(BASE_DIR)\n",
    "\n",
    "\n",
    "###############################\n",
    "## INITIAL OR NEW MNT DOWNLOAD:\n",
    "###############################\n",
    "## Install empty /mnt directory with input data and eventual output data\n",
    "# ! pip install gdown\n",
    "# ! gdown 1ok73if0F0oBlYycUr-rr3N_dXoOcXr3u\n",
    "# ! tar -xzvf *.tar.gz\n",
    "\n",
    "####################\n",
    "## SUBSEQUENT RUNS:\n",
    "####################\n",
    "# src_dir = os.path.join(BASE_DIR, 'confluence_empty')  # initial unzipped gdown \n",
    "run_dir = os.path.join(BASE_DIR, f'confluence_{RUN_NAME}') # new directory for run\n",
    "# shutil.copytree(src_dir, run_dir) # copy the contents of empty to new (preserves initial data)\n",
    "\n",
    "# p = Path(f\"{new_dir}/empty_mnt\") # rename internal mnt to run name\n",
    "# p.rename(p.with_name(f\"{RUN_NAME}_mnt\"))\n",
    "\n",
    "\n",
    "# Point to necessary directories \n",
    "SIF_DIR = os.path.join(run_dir, 'sif') # Store built Docker images\n",
    "sh_dir = os.path.join(run_dir, 'sh_scripts') # Store the sh scripts to run each module\n",
    "report_dir = os.path.join(run_dir, 'report') # Job logs\n",
    "mnt_dir = os.path.join(run_dir, f'{RUN_NAME}_mnt') #the mnt storing all confluence run data\n",
    "\n",
    "os.environ['RUN_NAME'] = RUN_NAME\n",
    "os.environ['BASE_DIR'] = BASE_DIR\n",
    "os.environ['REPO_DIR'] = REPO_DIR\n",
    "os.environ['SIF_DIR'] = SIF_DIR\n",
    "\n",
    "# Fail safe for directory build\n",
    "for d in [SIF_DIR, sh_dir, report_dir, REPO_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f'RUN_NAME: {RUN_NAME}')\n",
    "print(f'REPO_DIR: {REPO_DIR}')\n",
    "print(f'SIF_DIR: {SIF_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Name of confluence offline module\n",
    "#expanded and non_expanded modules work from 'setfinder' and 'combine_data'\n",
    "INCLUDED_MODULES = [\n",
    "    'expanded_setfinder',\n",
    "    'expanded_combine_data',\n",
    "    'input',\n",
    "    'non_expanded_setfinder',\n",
    "    'non_expanded_combine_data',\n",
    "    'prediagnostics',\n",
    "    # 'priors',\n",
    "    'metroman',\n",
    "    'metroman_consolidation',\n",
    "    'unconstrained_momma',\n",
    "    # 'hivdi',\n",
    "    # 'sad',\n",
    "    'sic4dvar',\n",
    "    'consensus',\n",
    "    # 'moi',\n",
    "    # 'unconstrained_offline',\n",
    "    # 'validation',\n",
    "    'output'\n",
    "]\n",
    "\n",
    "# Git modules to pull\n",
    "TARGET_MODULES = [\n",
    "    'setfinder',\n",
    "    'combine_data',\n",
    "    'input',\n",
    "    'prediagnostics',\n",
    "    # 'priors',\n",
    "    'metroman',\n",
    "    'metroman_consolidation',\n",
    "    'momma',\n",
    "    # 'hivdi',\n",
    "    # 'sad',\n",
    "    'sic4dvar',\n",
    "    'consensus',\n",
    "    # 'moi',\n",
    "    # 'offline',\n",
    "    # 'validation',\n",
    "    'output'\n",
    "]\n",
    "\n",
    "# Pull working branches for certain Git repos\n",
    "branch_map = {\n",
    "    'setfinder': 'main',\n",
    "    'combine_data': 'main',\n",
    "    'input': 'input_D_products',\n",
    "    'prediagnostics': 'main',\n",
    "    # 'priors': 'main',\n",
    "    'metroman': 'main',\n",
    "    'metroman_consolidation': 'main',\n",
    "    'momma': 'main',\n",
    "    # 'hivdi': 'main',\n",
    "    # 'sad': 'main',\n",
    "    'sic4dvar': 'main',\n",
    "    'consensus': 'main',\n",
    "    # 'moi': 'main',\n",
    "    # 'offline': 'main',\n",
    "    # 'validation': 'main',\n",
    "    'output': 'add-sword-version'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clone] Cloning setfinder from branch main...\n",
      "[Clone] Cloning combine_data from branch main...\n",
      "[Clone] Cloning input from branch input_D_products...\n",
      "[Clone] Cloning prediagnostics from branch main...\n",
      "[Clone] Cloning metroman from branch main...\n",
      "[Clone] Cloning metroman_consolidation from branch main...\n",
      "[Clone] Cloning momma from branch main...\n",
      "[Clone] Cloning sic4dvar from branch main...\n",
      "[Clone] Cloning consensus from branch main...\n",
      "[Clone] Cloning output from branch add-sword-version...\n"
     ]
    }
   ],
   "source": [
    "name_map = {\n",
    "        'offline': 'offline-discharge-data-product-creation',\n",
    "        'moi': 'MOI',\n",
    "        'validation': 'Validation'\n",
    "    } # docker images must be lower case\n",
    "\n",
    "clone_repos(repo_dir=REPO_DIR, repo_names=TARGET_MODULES, name_map=name_map, branch=branch_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build/Push modules to Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#------------------------------------------------\n",
    "\n",
    "# SETUP, DOCKER MUST BE OPEN ON COMPUTER\n",
    "\n",
    "push = True # Only select True if want to store images on dockerhub to move to HPC (you probably do)\n",
    "docker_username = 'efrie130'\n",
    "custom_tag_name = 'vD' # good to name same as the run, will default to 'latest'\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Generate Docker images from cloned modules\n",
    "\n",
    "build_and_push_images(\\\n",
    "                      repo_directory = REPO_DIR, \\\n",
    "                      modules_to_run = TARGET_MODULES, \\\n",
    "                      docker_username = docker_username, \\\n",
    "                      push = push, \\\n",
    "                      custom_tag_name = custom_tag_name \\\n",
    "                     )\n",
    "                      \n",
    "# The output should look something like \n",
    "# sha256:6900c3d99325a4a7c8b282d4a7a62f2a0f3fc673f03f5ca3333c2746bf20d06a\n",
    "# docker.io/travissimmons/setfinder:latest\n",
    "\n",
    "\n",
    "#command line version of build_and_push_images function above - verbose, remove push if not using DockerHub, change tag name as needed\n",
    "# ! docker build -t efrie130/validation:latest ./Validation/ && docker push efrie130/validation:latest & "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_local_run_scripts(\n",
    "    run: str,\n",
    "    modules_to_run: list,\n",
    "    script_jobs: dict,\n",
    "    base_dir: str,\n",
    "    repo_directory: str,\n",
    "    rebuild_docker: bool,\n",
    "    docker_username: str,\n",
    "    push: bool,\n",
    "    custom_tag_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate Python scripts to run Docker containers locally for each module.\n",
    "    Handles dynamic JSON file detection similar to SLURM version.\n",
    "    \"\"\"\n",
    "    # Directory structure\n",
    "    mnt_dir = os.path.join(base_dir, f'confluence_{run}', f'{run}_mnt')\n",
    "    input_dir = os.path.join(mnt_dir, 'input')\n",
    "    sh_dir = os.path.join(base_dir, f'confluence_{run}', 'sh_scripts')\n",
    "    os.makedirs(sh_dir, exist_ok=True)\n",
    "    \n",
    "    # JSON file paths (similar to HPC version)\n",
    "    json_files = {\n",
    "        'reaches_of_interest': os.path.join(input_dir, 'reaches_of_interest.json'),\n",
    "        'expanded': os.path.join(input_dir, 'expanded_reaches_of_interest.json'),\n",
    "        'reaches': os.path.join(input_dir, 'reaches.json'),\n",
    "        'basin': os.path.join(input_dir, 'basin.json'),\n",
    "        'metrosets': os.path.join(input_dir, 'metrosets.json'),\n",
    "    }\n",
    "    \n",
    "    # Build Docker images if requested\n",
    "    if rebuild_docker:\n",
    "        print(\"Building Docker images...\")\n",
    "        build_and_push_images(\n",
    "            repo_directory=repo_directory,\n",
    "            target_modules=modules_to_run,\n",
    "            docker_username=docker_username,\n",
    "            push=push,\n",
    "            custom_tag_name=custom_tag_name\n",
    "        )\n",
    "    \n",
    "    # Command dictionary - DO NOT CHANGE\n",
    "    command_dict = {\n",
    "        'expanded_setfinder': f'docker run --rm -v {mnt_dir}/input:/data {docker_username}/setfinder:{custom_tag_name} -r reaches_of_interest.json -c continent.json -e -s 17 -o /data -n /data -a MetroMan HiVDI SIC -i {{index}}',\n",
    "        'expanded_combine_data': f'docker run --rm -v {mnt_dir}/input:/data {docker_username}/combine_data:{custom_tag_name} -d /data -e -s 17',\n",
    "        'input': f'docker run --rm -v {mnt_dir}/input:/mnt/data {docker_username}/input:{custom_tag_name} -v 17 -r /mnt/data/expanded_reaches_of_interest.json -c SWOT_L2_HR_RiverSP_D -i {{index}}',\n",
    "        'non_expanded_setfinder': f'docker run --rm -v {mnt_dir}/input:/data {docker_username}/setfinder:{custom_tag_name} -c continent.json -s 17 -o /data -n /data -a MetroMan HiVDI SIC -i {{index}}',\n",
    "        'non_expanded_combine_data': f'docker run --rm -v {mnt_dir}/input:/data {docker_username}/combine_data:{custom_tag_name} -d /data -s 17',\n",
    "        'prediagnostics': f'docker run --rm -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/diagnostics/prediagnostics:/mnt/data/output {docker_username}/prediagnostics:{custom_tag_name} -r reaches.json -i {{index}}',\n",
    "        'unconstrained_priors': f'docker run --rm -v {mnt_dir}/input:/mnt/data {docker_username}/priors:{custom_tag_name} -r unconstrained -p usgs riggs -g -s local -i {{index}}',\n",
    "        'constrained_priors': f'docker run --rm -v {mnt_dir}/input:/mnt/data {docker_username}/priors:{custom_tag_name} -r constrained -p usgs riggs -g -s local -i {{index}}',\n",
    "        'metroman': f'docker run --rm --env AWS_BATCH_JOB_ID=\"foo\" -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/metroman:/mnt/data/output {docker_username}/metroman:{custom_tag_name} -r metrosets.json -s local -v -i {{index}}',\n",
    "        'metroman_consolidation': f'docker run --rm -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/metroman:/mnt/data/flpe {docker_username}/metroman_consolidation:{custom_tag_name} -i {{index}}',\n",
    "        'unconstrained_momma': f'docker run --rm -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/momma:/mnt/data/output {docker_username}/momma:{custom_tag_name} -r reaches.json -m 3 -i {{index}}',\n",
    "        'constrained_momma': f'docker run --rm -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/momma:/mnt/data/output {docker_username}/momma:{custom_tag_name} -r reaches.json -m 3 -c -i {{index}}',\n",
    "        'neobam': f'docker run --rm -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/geobam:/mnt/data/output {docker_username}/neobam:{custom_tag_name} -r reaches.json -i {{index}}',\n",
    "        'sad': f'docker run --rm -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/sad:/mnt/data/output {docker_username}/sad:{custom_tag_name} --reachfile reaches.json --index {{index}}',\n",
    "        'sic4dvar': f'docker run --rm -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/sic4dvar:/mnt/data/output -v {mnt_dir}/logs:/mnt/data/logs {docker_username}/sic4dvar:{custom_tag_name} -r reaches.json --index {{index}}',\n",
    "        'moi': f'docker run --rm --env AWS_BATCH_JOB_ID=\"foo\" -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/output {docker_username}/moi:{custom_tag_name} -j basin.json -v -b unconstrained -i {{index}}',\n",
    "        'consensus': f'docker run --rm -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe {docker_username}/consensus:{custom_tag_name} --mntdir /mnt/data -r /mnt/data/input/reaches.json -i {{index}}',\n",
    "        'unconstrained_offline': f'docker run --rm -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/offline:/mnt/data/output {docker_username}/offline:{custom_tag_name} unconstrained timeseries integrator reaches.json {{index}}',\n",
    "        'validation': f'docker run --rm -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/offline:/mnt/data/offline -v {mnt_dir}/validation:/mnt/data/output {docker_username}/validation:{custom_tag_name} reaches.json unconstrained {{index}}',\n",
    "        'output': f'docker run --rm -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/diagnostics:/mnt/data/diagnostics -v {mnt_dir}/offline:/mnt/data/offline -v {mnt_dir}/validation:/mnt/data/validation -v {mnt_dir}/output:/mnt/data/output {docker_username}/output:{custom_tag_name} -s local -j /app/metadata/metadata.json -m input prediagnostics momma metroman sic4dvar consensus swot priors -v 17 -i {{index}}'\n",
    "    }\n",
    "    \n",
    "    output_paths = []\n",
    "    \n",
    "    for module in modules_to_run:\n",
    "        if module not in command_dict:\n",
    "            print(f\"Warning: No command defined for module '{module}', skipping\")\n",
    "            continue\n",
    "        \n",
    "        job_count = script_jobs.get(module, \"1\")\n",
    "        \n",
    "        # Generate Python script with dynamic job count detection\n",
    "        # Generate Python script with dynamic job count detection\n",
    "        script_content = f'''#!/usr/bin/env python3\n",
    "import subprocess as sp\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Module: {module}\n",
    "\n",
    "# JSON file paths\n",
    "json_files = {{\n",
    "    'reaches_of_interest': r'{json_files['reaches_of_interest']}',\n",
    "    'expanded': r'{json_files['expanded']}',\n",
    "    'reaches': r'{json_files['reaches']}',\n",
    "    'basin': r'{json_files['basin']}',\n",
    "    'metrosets': r'{json_files['metrosets']}',\n",
    "}}\n",
    "\n",
    "def get_json_length(filepath):\n",
    "    \"\"\"Get length of JSON array file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                return len(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {{{{filepath}}}}: {{{{e}}}}\")\n",
    "    return None\n",
    "\n",
    "# Determine job count for this module\n",
    "job_count = \"{job_count}\"\n",
    "\n",
    "if job_count == \"$default_jobs\":\n",
    "    # Dynamic job count based on module-specific logic\n",
    "    num_jobs = None\n",
    "    \n",
    "    # Module-specific JSON file selection (matching HPC logic)\n",
    "    if \"{module}\" == \"input\":\n",
    "        # Use expanded_reaches_of_interest.json if it exists\n",
    "        num_jobs = get_json_length(json_files['expanded'])\n",
    "        if num_jobs is None:\n",
    "            print(\"Error: expanded_reaches_of_interest.json not found for input module\")\n",
    "            print(\"Make sure expanded_combine_data has been run first\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    elif \"{module}\" in [\"metroman\", \"metroman_consolidation\"]:\n",
    "        # Use metrosets.json if it exists, otherwise reaches.json, otherwise reaches_of_interest.json\n",
    "        num_jobs = get_json_length(json_files['metrosets'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches_of_interest'])\n",
    "    \n",
    "    elif \"{module}\" == \"moi\":\n",
    "        # Use basin.json if it exists, otherwise reaches.json, otherwise reaches_of_interest.json\n",
    "        num_jobs = get_json_length(json_files['basin'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches_of_interest'])\n",
    "    \n",
    "    else:\n",
    "        # For most modules: use reaches.json if exists, otherwise reaches_of_interest.json\n",
    "        num_jobs = get_json_length(json_files['reaches'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches_of_interest'])\n",
    "    \n",
    "    if num_jobs is None:\n",
    "        print(\"Error: Could not determine job count for module '{module}'\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"Determined {{{{num_jobs}}}} job(s) dynamically for module '{module}'\")\n",
    "else:\n",
    "    num_jobs = int(job_count)\n",
    "    print(f\"Using hardcoded job count: {{{{num_jobs}}}} for module '{module}'\")\n",
    "\n",
    "# Docker command template\n",
    "command_template = \"\"\"{command_dict[module]}\"\"\"\n",
    "\n",
    "print(f\"\\\\nStarting module: {module}\")\n",
    "print(f\"Running {{{{num_jobs}}}} job(s)\\\\n\")\n",
    "\n",
    "for index in range(num_jobs):\n",
    "    print(f\"--- Running job {{{{index + 1}}}}/{{{{num_jobs}}}} for module '{module}' ---\")\n",
    "    \n",
    "    # Replace {{{{{{{{index}}}}}}}} with actual index\n",
    "    run_command = command_template.replace('{{{{index}}}}', str(index))\n",
    "    print(f\"Command: {{{{run_command}}}}\")\n",
    "    \n",
    "    try:\n",
    "        result = sp.run(run_command, shell=True, check=True)\n",
    "        print(f\"✓ Job {{{{index}}}} completed successfully\\\\n\")\n",
    "    except sp.CalledProcessError as e:\n",
    "        print(f\"✗ Job {{{{index}}}} failed with exit code {{{{e.returncode}}}}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "print(f\"✓ All jobs completed for module '{module}'\")\n",
    "'''\n",
    "\n",
    "        output_script_path = os.path.join(sh_dir, f\"run_{module}.py\")\n",
    "        with open(output_script_path, 'w') as f:\n",
    "            f.write(script_content)\n",
    "        \n",
    "        os.chmod(output_script_path, 0o755)\n",
    "        output_paths.append(output_script_path)\n",
    "        print(f\"Created: {output_script_path}\")\n",
    "    \n",
    "    return output_paths\n",
    "\n",
    "def generate_run_all_modules_script(\n",
    "    run: str,\n",
    "    modules_to_run: list,\n",
    "    script_jobs: dict,\n",
    "    base_dir: str,\n",
    "    script_name: str = \"run_all_modules.sh\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a bash script that runs all module scripts in series.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    run : str\n",
    "        Run name\n",
    "    modules_to_run : list\n",
    "        List of modules\n",
    "    script_jobs : dict\n",
    "        Module -> job count mapping\n",
    "    base_dir : str\n",
    "        Base directory\n",
    "    script_name : str\n",
    "        Name of the generated script\n",
    "    \"\"\"\n",
    "    sh_dir = os.path.join(base_dir, f'confluence_{run}', 'sh_scripts')\n",
    "    script_path = os.path.join(base_dir, f'confluence_{run}', script_name)\n",
    "    \n",
    "    # Filter modules with non-zero job counts\n",
    "    filtered_modules = [m for m in modules_to_run if script_jobs.get(m, \"0\") != \"0\"]\n",
    "    \n",
    "    # Generate modules array\n",
    "    modules_array = \"modules_to_run=(\\n\"\n",
    "    for module in filtered_modules:\n",
    "        modules_array += f'    \"{module}\"\\n'\n",
    "    modules_array += \")\\n\"\n",
    "    \n",
    "    script_content = f\"\"\"#!/bin/bash\n",
    "# {script_name}\n",
    "# Runs all module scripts in series for run: {run}\n",
    "\n",
    "SCRIPT_DIR=\"{sh_dir}\"\n",
    "\n",
    "{modules_array}\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Starting Confluence Run: {run}\"\n",
    "echo \"==========================================\"\n",
    "echo \"\"\n",
    "\n",
    "for module in \"${{modules_to_run[@]}}\"; do\n",
    "    script_path=\"${{SCRIPT_DIR}}/run_${{module}}.py\"\n",
    "    \n",
    "    if [[ -f \"$script_path\" ]]; then\n",
    "        echo \"==========================================\"\n",
    "        echo \"Running module: $module\"\n",
    "        echo \"Script: $script_path\"\n",
    "        echo \"==========================================\"\n",
    "        python3 \"$script_path\"\n",
    "        \n",
    "        if [[ $? -ne 0 ]]; then\n",
    "            echo \"Error occurred while running $module. Exiting.\"\n",
    "            exit 1\n",
    "        fi\n",
    "        \n",
    "        echo \"Finished module: $module\"\n",
    "        echo \"\"\n",
    "    else\n",
    "        echo \"Script not found for module: $module. Skipping.\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"All modules completed successfully!\"\n",
    "echo \"==========================================\"\n",
    "\"\"\"\n",
    "    \n",
    "    with open(script_path, \"w\") as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    os.chmod(script_path, 0o755)\n",
    "    print(f\"Created: {script_path}\")\n",
    "    return script_path\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\\run_expanded_setfinder.py\n",
      "Created: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\\run_expanded_combine_data.py\n",
      "Created: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\\run_input.py\n",
      "Created: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\\run_non_expanded_setfinder.py\n",
      "Created: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\\run_non_expanded_combine_data.py\n",
      "Created: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\\run_prediagnostics.py\n",
      "Created: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\\run_metroman.py\n",
      "Created: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\\run_metroman_consolidation.py\n",
      "Created: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\\run_unconstrained_momma.py\n",
      "Created: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\\run_sic4dvar.py\n",
      "Created: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\\run_consensus.py\n",
      "Created: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\\run_output.py\n",
      "Created: C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\run_all_modules.sh\n",
      "\n",
      "All scripts generated!\n",
      "\n",
      "To run all modules:\n",
      "  cd C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\n",
      "  ./run_all_modules.sh\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define hardcoded job counts\n",
    "HARDCODED_JOBS = {\n",
    "    \"expanded_setfinder\": \"7\",\n",
    "    \"expanded_combine_data\": \"1\",\n",
    "    \"non_expanded_setfinder\": \"7\",\n",
    "    \"non_expanded_combine_data\": \"1\",\n",
    "    \"unconstrained_priors\": \"7\",\n",
    "    \"constrained_priors\": \"7\",\n",
    "    \"output\": \"7\",\n",
    "}\n",
    "\n",
    "# Define dynamic modules\n",
    "DYNAMIC_MODULES = [\n",
    "    \"input\",\n",
    "    \"prediagnostics\",\n",
    "    \"metroman\",\n",
    "    \"metroman_consolidation\",\n",
    "    \"sic4dvar\",\n",
    "    \"unconstrained_momma\",\n",
    "    \"constrained_momma\",\n",
    "    \"sad\",\n",
    "    \"moi\",\n",
    "    \"consensus\",\n",
    "    \"unconstrained_offline\",\n",
    "    \"validation\",\n",
    "]\n",
    "\n",
    "# Build script_jobs dict\n",
    "script_jobs = {}\n",
    "for module in INCLUDED_MODULES:\n",
    "    if module in HARDCODED_JOBS:\n",
    "        script_jobs[module] = HARDCODED_JOBS[module]\n",
    "    elif module in DYNAMIC_MODULES:\n",
    "        script_jobs[module] = \"$default_jobs\"\n",
    "\n",
    "# Generate scripts\n",
    "generate_local_run_scripts(\n",
    "    run=RUN_NAME,\n",
    "    modules_to_run=INCLUDED_MODULES,\n",
    "    script_jobs=script_jobs,\n",
    "    base_dir=BASE_DIR,\n",
    "    repo_directory=REPO_DIR,\n",
    "    rebuild_docker=False,\n",
    "    docker_username=docker_username,\n",
    "    push=False,\n",
    "    custom_tag_name=custom_tag_name\n",
    ")\n",
    "\n",
    "# Generate master run script\n",
    "generate_run_all_modules_script(\n",
    "    run=RUN_NAME,\n",
    "    modules_to_run=INCLUDED_MODULES,\n",
    "    script_jobs=script_jobs,\n",
    "    base_dir=BASE_DIR,\n",
    "    script_name=\"run_all_modules.sh\"\n",
    ")\n",
    "\n",
    "print(\"\\nAll scripts generated!\")\n",
    "print(f\"\\nTo run all modules:\")\n",
    "print(f\"  cd {os.path.join(BASE_DIR, f'confluence_{RUN_NAME}')}\")\n",
    "print(f\"  ./run_all_modules.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 1E96-2F26\n",
      "\n",
      " Directory of C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\n",
      "\n",
      "02/04/2026  12:50 PM    <DIR>          .\n",
      "02/04/2026  12:49 PM    <DIR>          ..\n",
      "02/04/2026  01:48 PM    <DIR>          confluence_Dtest\n",
      "02/04/2026  12:50 PM    <DIR>          modules\n",
      "               0 File(s)              0 bytes\n",
      "               4 Dir(s)  13,928,333,312 bytes free\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "!dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\n",
      " Volume in drive C is OS\n",
      " Volume Serial Number is 1E96-2F26\n",
      "\n",
      " Directory of C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts\n",
      "\n",
      "02/04/2026  01:48 PM    <DIR>          .\n",
      "02/04/2026  01:48 PM    <DIR>          ..\n",
      "02/04/2026  01:48 PM             5,554 run_consensus.py\n",
      "02/04/2026  01:48 PM             5,481 run_expanded_combine_data.py\n",
      "02/04/2026  01:48 PM             5,536 run_expanded_setfinder.py\n",
      "02/04/2026  01:48 PM             5,400 run_input.py\n",
      "02/04/2026  01:48 PM             5,562 run_metroman.py\n",
      "02/04/2026  01:48 PM             5,655 run_metroman_consolidation.py\n",
      "02/04/2026  01:48 PM             5,518 run_non_expanded_combine_data.py\n",
      "02/04/2026  01:48 PM             5,545 run_non_expanded_setfinder.py\n",
      "02/04/2026  01:48 PM             6,258 run_output.py\n",
      "02/04/2026  01:48 PM             5,598 run_prediagnostics.py\n",
      "02/04/2026  01:48 PM             5,654 run_sic4dvar.py\n",
      "02/04/2026  01:48 PM             5,628 run_unconstrained_momma.py\n",
      "              12 File(s)         67,389 bytes\n",
      "               2 Dir(s)  13,938,421,760 bytes free\n"
     ]
    }
   ],
   "source": [
    "base = os.path.abspath(\"confluence_Dtest/sh_scripts\")\n",
    "print(base)\n",
    "!dir \"{base}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts/run_expanded_setfinder.py\", line 2\n",
      "    import subprocess as sp\n",
      "IndentationError: unexpected indent\n",
      "  File \"C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts/run_expanded_combine_data.py\", line 2\n",
      "    import subprocess as sp\n",
      "IndentationError: unexpected indent\n",
      "  File \"C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts/run_input.py\", line 2\n",
      "    import subprocess as sp\n",
      "IndentationError: unexpected indent\n",
      "  File \"C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts/run_prediagnostics.py\", line 2\n",
      "    import subprocess as sp\n",
      "IndentationError: unexpected indent\n",
      "  File \"C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts/run_metroman.py\", line 2\n",
      "    import subprocess as sp\n",
      "IndentationError: unexpected indent\n",
      "  File \"C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts/run_metroman_consolidation.py\", line 2\n",
      "    import subprocess as sp\n",
      "IndentationError: unexpected indent\n",
      "  File \"C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts/run_unconstrained_momma.py\", line 2\n",
      "    import subprocess as sp\n",
      "IndentationError: unexpected indent\n",
      "  File \"C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts/run_sic4dvar.py\", line 2\n",
      "    import subprocess as sp\n",
      "IndentationError: unexpected indent\n",
      "  File \"C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts/run_consensus.py\", line 2\n",
      "    import subprocess as sp\n",
      "IndentationError: unexpected indent\n",
      "  File \"C:\\Users\\efriedmann\\OneDrive - University of Massachusetts\\Documents\\confluence\\confluence_Dtest\\sh_scripts/run_output.py\", line 2\n",
      "    import subprocess as sp\n",
      "IndentationError: unexpected indent\n"
     ]
    }
   ],
   "source": [
    "for mod in [\n",
    "    'expanded_setfinder','expanded_combine_data','input','prediagnostics',\n",
    "    'metroman','metroman_consolidation','unconstrained_momma',\n",
    "    'sic4dvar','consensus','output'\n",
    "]:\n",
    "    !python \"{base}/run_{mod}.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "script_jobs = {\n",
    "        \"expanded_setfinder\": \"7\",\n",
    "        \"expanded_combine_data\": \"1\",\n",
    "        \"input\": \"$default_jobs\",\n",
    "        \"non_expanded_setfinder\": \"7\",\n",
    "        \"non_expanded_combine_data\": \"1\",\n",
    "        \"prediagnostics\": \"$default_jobs\",\n",
    "        # \"unconstrained_priors\": \"7\", \n",
    "        \"sad\": \"$default_jobs\",\n",
    "        \"metroman\": \"$default_jobs\",\n",
    "        \"metroman_consolidation\": \"$default_jobs\",\n",
    "        \"sic4dvar\": \"$default_jobs\",\n",
    "        \"unconstrained_momma\": \"$default_jobs\",\n",
    "        \"neobam\": \"$default_jobs\",\n",
    "        \"moi\": \"$default_jobs\",\n",
    "        \"unconstrained_offline\": \"$default_jobs\",\n",
    "        \"validation\": \"$default_jobs\",\n",
    "        \"output\": \"7\",\n",
    "    }\n",
    "\n",
    "# Then, generate the Python script that runs the command\n",
    "# Change Rebuild docker to TRUE if you made module and tag changes for a one-click run \n",
    "\n",
    "generate_run_scripts(\n",
    "    run=run,\n",
    "    modules_to_run=modules_to_run,\n",
    "    script_jobs=script_jobs,\n",
    "    base_dir=base_dir,\n",
    "    repo_directory=repo_directory,\n",
    "    rebuild_docker=False,\n",
    "    docker_username=docker_username,\n",
    "    push=False,\n",
    "    custom_tag_name=run,\n",
    "    default_jobs_json_path=os.path.join(base_dir, f\"confluence_{run}\", f\"{run}_mnt\", \"input\", \"expanded_reaches_of_interest.json\") #note that expanded_reaches_of_interest is only generated after expanded_combine_data is run\n",
    ")\n",
    "\n",
    "generate_run_all_modules_bash(\n",
    "    modules_to_run=modules_to_run,\n",
    "    script_jobs=script_jobs,\n",
    "    base_dir=base_dir,\n",
    "    script_folder_name = \"sh_scripts\",\n",
    "    bash_script_name = \"run_all_modules.sh\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running this notebook, there will be individual python scripts and run_all_module.sh file generated in the same directory.\n",
    "# You can either run the python scripts individually or run the .sh script to run them all on the command line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
