{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confluence Module Docker Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Confluence on LOCAL Machine\n",
    "\n",
    "# Requirements\n",
    "* docker installed somewhere where you have sudo priveledges to the point where \"docker --version\" completes successfully\n",
    "* a dockerhub account (free)\n",
    "* a python environment\n",
    "\n",
    "\n",
    "# Overall Tasks\n",
    "* Git clone all of the repos you want to run to a machine where you have sudo priveledges and where \"docker --version\" works (locally)\n",
    "* Prep an empty_mnt directory to store confluence run (requires gdown package in environment)\n",
    "* Run the \"build_and_push_images\" function of this notebook locally\n",
    "* Run the \"generate_run_scripts\" section of this notebook to create python submission scripts for each module\n",
    "* Run the generate_all_modules_bash Confluence Driver Script Generator section of this notebook  to create a .sh submission script that runs each of the modules one by one (the one click run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp\n",
    "\n",
    "# FUNCTIONS IGNORE\n",
    "def build_and_push_images(repo_directory:str, modules_to_run:list, docker_username:str, push:bool = True, custom_tag_name:str = 'latest'):\n",
    "    for a_repo_name in modules_to_run:\n",
    "        repo_path = os.path.join(repo_directory, a_repo_name)\n",
    "        docker_path = f'{docker_username}/{a_repo_name}:{custom_tag_name}'\n",
    "        build_cmd = ['docker', 'build','--quiet', '-f', os.path.join(repo_path, \"Dockerfile\"), '-t', docker_path, repo_path]\n",
    "        try:\n",
    "            sp.run(build_cmd)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Docker build failed...\\n\"\n",
    "                f\"Build Command: {build_cmd}\\n\"\n",
    "                f\"Error: {e}\"\n",
    "            )\n",
    "        if push:\n",
    "            try:\n",
    "                push_cmd = ['docker', 'push','--quiet', docker_path]\n",
    "                sp.run(push_cmd)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Docker push failed...\\n\"\n",
    "                    f\"Push Command: {push_cmd}\\n\"\n",
    "                    f\"Error: {e}\"\n",
    "                )\n",
    "\n",
    "import os\n",
    "import json\n",
    "import subprocess as sp\n",
    "\n",
    "def generate_run_scripts(\n",
    "    run,\n",
    "    modules_to_run,          # list[str]\n",
    "    script_jobs,             # dict of module -> job count (str), e.g., \"7\" or \"$default_jobs\"\n",
    "    base_dir,\n",
    "    repo_directory,\n",
    "    rebuild_docker,\n",
    "    docker_username,\n",
    "    push,\n",
    "    custom_tag_name,\n",
    "    default_jobs_json_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a separate Python run script for each module in modules_to_run.\n",
    "    If a module's job count is \"$default_jobs\", dynamically determine the number\n",
    "    of jobs from the length of the JSON list at default_jobs_json_path.\n",
    "\n",
    "    Parameters:\n",
    "    - run (str): Name of the confluence run\n",
    "    - modules_to_run (list[str]): Modules to generate scripts for.\n",
    "    - script_jobs (dict): Module -> job count (int as str or \"$default_jobs\").\n",
    "    - base_dir (str): Path to repos.\n",
    "    - rebuild_docker (bool): Whether to rebuild docker images.\n",
    "    - default_jobs_json_path (str): JSON file path to determine default_jobs length.\n",
    "    \"\"\"\n",
    "    # Has to exist with 'mnt' structure (Doit exister avec la structure 'mnt')\n",
    "    mnt_dir = os.path.join(base_dir, f'confluence_{run}', f'{run}_mnt')\n",
    "    \n",
    "    # Create the sh_scripts directory (Cree le repertoire sh_scripts)\n",
    "    sh_dir = os.path.join(base_dir, f'confluence_{run}', 'sh_scripts')\n",
    "    if not os.path.exists(sh_dir):\n",
    "        os.makedirs(sh_dir)\n",
    "    \n",
    "    # Create the sif directory (Cree la repertoire sif)\n",
    "    sif_dir = os.path.join(base_dir, f'confluence_{run}', 'sif')\n",
    "    if not os.path.exists(sif_dir):\n",
    "        os.makedirs(sif_dir)\n",
    "    \n",
    "    # Create the report directory (Cree la repertoire report)\n",
    "    report_dir = os.path.join(base_dir, f'confluence_{run}', 'report')\n",
    "    if not os.path.exists(report_dir):\n",
    "        os.makedirs(report_dir)\n",
    "\n",
    "    # This is a dictionary of all of the Confluence module run commands translated to singularity run commands.\n",
    "    # You should not have to change anything here.\n",
    "    command_dict = {\n",
    "        'expanded_setfinder': f'docker run -v {mnt_dir}/input:/data setfinder -r reaches_of_interest.json -c continent.json -e -s 16 -o /data -n /data -a MetroMan HiVDI SIC NeoBAM -i index_to_run',\n",
    "        'expanded_combine_data': f'docker run -v {mnt_dir}/input:/data combine_data -d /data -e -s 16',\n",
    "        'input': f'docker run -v {mnt_dir}/input:/mnt/data input -r /mnt/data/expanded_reaches_of_interest.json -i index_to_run',\n",
    "        'non_expanded_setfinder': f'docker run -v {mnt_dir}/input:/data setfinder -c continent.json -s 16 -o /data -n /data -a MetroMan HiVDI SIC NeoBAM -i index_to_run',\n",
    "        'non_expanded_combine_data': f'docker run -v {mnt_dir}/input:/data combine_data -d /data -s 16',\n",
    "        'prediagnostics': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/diagnostics/prediagnostics:/mnt/data/output prediagnostics -r reaches.json -i index_to_run',\n",
    "        'unconstrained_priors': f'docker run -v {mnt_dir}/input:/mnt/data priors -r unconstrained -p usgs riggs -g -s local -i index_to_run', # Branch local_run\n",
    "        'metroman': f'docker run --env AWS_BATCH_JOB_ID=\"foo\" -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/metroman:/mnt/data/output metroman -r metrosets.json -s local -v -i index_to_run', # branch local_run_args\n",
    "        'metroman_consolidation': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/metroman:/mnt/data/flpe metroman_consolidation -i index_to_run',\n",
    "        'unconstrained_momma': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/momma:/mnt/data/output momma -r reaches.json -m 3 -i index_to_run',\n",
    "        'neobam': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/geobam:/mnt/data/output neobam -r reaches.json -i index_to_run',\n",
    "        'sad': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/sad:/mnt/data/output sad --reachfile reaches.json --index index_to_run',\n",
    "        'MOI': f'docker run --env AWS_BATCH_JOB_ID=\"foo\" -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/output moi -j basin.json -v -b unconstrained -s local -i index_to_run',\n",
    "        'unconstrained_offline': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/offline:/mnt/data/output offline unconstrained timeseries integrator reaches.json index_to_run',\n",
    "        'Validation': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/offline:/mnt/data/offline -v {mnt_dir}/validation:/mnt/data/output validation reaches.json unconstrained index_to_run',\n",
    "        'output': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/diagnostics:/mnt/data/diagnostics -v {mnt_dir}/offline:/mnt/data/offline -v {mnt_dir}/validation:/mnt/data/validation -v {mnt_dir}/output:/mnt/data/output output -s local -j /app/metadata/metadata.json -m input priors prediagnostics momma neobam metroman sic4dvar sad moi offline validation swot -i index_to_run'\n",
    "    }\n",
    "    \n",
    "    # Build docker images once if requested\n",
    "    if rebuild_docker:\n",
    "        build_and_push_images(\n",
    "            repo_directory=repo_directory,\n",
    "            modules_to_run=modules_to_run,\n",
    "            docker_username=docker_username,\n",
    "            push=push,\n",
    "            custom_tag_name=custom_tag_name\n",
    "        )\n",
    "\n",
    "    # Load default_jobs from JSON if needed\n",
    "    default_jobs = None\n",
    "    if \"$default_jobs\" in script_jobs.values():\n",
    "        if default_jobs_json_path is None:\n",
    "            raise ValueError(\"default_jobs_json_path must be provided when using $default_jobs\")\n",
    "        with open(default_jobs_json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                default_jobs = len(data)\n",
    "            else:\n",
    "                raise ValueError(f\"Expected a JSON list at {default_jobs_json_path}\")\n",
    "\n",
    "    output_paths = []\n",
    "\n",
    "    for module in modules_to_run:\n",
    "        job_count = script_jobs.get(module, \"1\")\n",
    "        if job_count == \"$default_jobs\":\n",
    "            if default_jobs is None:\n",
    "                raise ValueError(f\"default_jobs JSON not loaded for module {module}\")\n",
    "            start_index = 0\n",
    "            end_index = default_jobs - 1\n",
    "        else:\n",
    "            start_index = 0\n",
    "            end_index = int(job_count) - 1\n",
    "\n",
    "        script_content = f\"\"\"#!/usr/bin/env python3\n",
    "import subprocess as sp\n",
    "\n",
    "# Docker command for module: {module}\n",
    "command = f'{command_dict[module]}'\n",
    "\n",
    "for index in range({start_index}, {end_index} + 1):\n",
    "    print(f\"Running command for module '{module}' at index {{index}}\")\n",
    "    run_command = command.replace('index_to_run', str(index))\n",
    "    sp.run(run_command, shell=True, check=True)\n",
    "\"\"\"\n",
    "\n",
    "        output_script_path = os.path.join(sh_dir, f\"run_{module}.py\")\n",
    "        with open(output_script_path, 'w') as f:\n",
    "            f.write(script_content)\n",
    "\n",
    "        os.chmod(output_script_path, 0o755)\n",
    "        output_paths.append(output_script_path)\n",
    "        print(f\"Python script created: {output_script_path}\")\n",
    "\n",
    "    return output_paths\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def generate_run_all_modules_bash(\n",
    "    modules_to_run: list[str],\n",
    "    script_jobs: dict,\n",
    "    base_dir: str,\n",
    "    script_folder_name: str = \"sh_scripts\",\n",
    "    bash_script_name: str = \"run_all_modules.sh\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates a Bash script that runs all per-module scripts in series.\n",
    "    Only includes modules for which a script was actually generated.\n",
    "\n",
    "    Parameters:\n",
    "    - modules_to_run: list of module names\n",
    "    - script_jobs: dict of module -> job count (str), used to filter modules\n",
    "    - base_dir: directory to save the bash script\n",
    "    - script_folder_name: folder inside base_dir where run_<module>.py scripts reside\n",
    "    - bash_script_name: name of the generated bash script\n",
    "\n",
    "    Returns:\n",
    "    - full path to the generated bash script\n",
    "    \"\"\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    bash_script_path = os.path.join(base_dir, f'confluence_{run}', bash_script_name)\n",
    "\n",
    "    # Only include modules that have non-zero job counts\n",
    "    filtered_modules = []\n",
    "    for module in modules_to_run:\n",
    "        count = script_jobs.get(module, \"0\")\n",
    "        if count != \"0\":\n",
    "            filtered_modules.append(module)\n",
    "\n",
    "    # Generate modules array string\n",
    "    modules_array_str = \"modules_to_run=(\\n\"\n",
    "    for module in filtered_modules:\n",
    "        modules_array_str += f\"    \\\"{module}\\\"\\n\"\n",
    "    modules_array_str += \")\\n\"\n",
    "\n",
    "    script_content = f\"\"\"#!/bin/bash\n",
    "# {bash_script_name}\n",
    "# Runs all generated module scripts in series\n",
    "\n",
    "SCRIPT_DIR=\"{script_folder_name}\"\n",
    "\n",
    "{modules_array_str}\n",
    "\n",
    "echo \"Starting module runs...\"\n",
    "\n",
    "for module in \"${{modules_to_run[@]}}\"; do\n",
    "    script_path=\"${{SCRIPT_DIR}}/run_${{module}}.py\"\n",
    "\n",
    "    if [[ -f \"$script_path\" ]]; then\n",
    "        echo \"---------------------------------------------\"\n",
    "        echo \"Running module: $module\"\n",
    "        echo \"Script: $script_path\"\n",
    "        echo \"---------------------------------------------\"\n",
    "        python3 \"$script_path\"\n",
    "        if [[ $? -ne 0 ]]; then\n",
    "            echo \"Error occurred while running $module. Exiting.\"\n",
    "            exit 1\n",
    "        fi\n",
    "        echo \"Finished module: $module\"\n",
    "        echo\n",
    "    else\n",
    "        echo \"Script not found for module: $module. Skipping.\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"All modules completed successfully.\"\n",
    "\"\"\"\n",
    "\n",
    "    with open(bash_script_path, \"w\") as f:\n",
    "        f.write(script_content)\n",
    "\n",
    "    os.chmod(bash_script_path, 0o755)\n",
    "    print(f\"Generated bash script: {bash_script_path}\")\n",
    "    return bash_script_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import subprocess as sp\n",
    "run = 'run1'\n",
    "\n",
    "base_dir = '/Users/elisafriedmann/Library/CloudStorage/OneDrive-UniversityofMassachusetts/UMass/SWOT/confluence/run-confluence-locally/'  # Downloaded using: gdown 1xRltFZ1gyP_nvwHMJW-rIgClzXx8CSLC\n",
    "\n",
    "repo_directory = '/home/travis/repos' # Contains the github cloned repos of the modules you want to run, check documentation for branches\n",
    "\n",
    "\n",
    "\n",
    "script_jobs = {\n",
    "        \"expanded_setfinder\": \"7\",\n",
    "        \"expanded_combine_data\": \"1\",\n",
    "        \"input_so\": \"$default_jobs\",\n",
    "        \"non_expanded_setfinder\": \"7\",\n",
    "        \"non_expanded_combine_data\": \"1\",\n",
    "        \"prediagnostics_permissive\": \"$default_jobs\",\n",
    "        # \"unconstrained_priors\": \"7\", \n",
    "        \"sad\": \"$default_jobs\",\n",
    "        \"metroman\": \"$default_jobs\",\n",
    "        \"metroman_consolidation\": \"$default_jobs\",\n",
    "        \"sic4dvar\": \"$default_jobs\",\n",
    "        \"unconstrained_momma\": \"$default_jobs\",\n",
    "        \"neobam\": \"$default_jobs\",\n",
    "        \"moi\": \"$default_jobs\",\n",
    "        \"unconstrained_offline\": \"$default_jobs\",\n",
    "        \"validation\": \"$default_jobs\",\n",
    "        \"output\": \"7\",\n",
    "    }\n",
    "\n",
    "\n",
    "#------------------------------------------------\n",
    "\n",
    "# SETUP\n",
    "\n",
    "modules_to_run = ['expanded_setfinder', 'expanded_combine_data', 'input'] # Chose what module to run using the below command dict, they are listed in order. eg: run expanded_setfinder first\n",
    "\n",
    "# Only provide this if you want to store images on dockerhub to move to HPC (you probably do)\n",
    "push = True\n",
    "docker_username = 'mydockerusername'\n",
    "custom_tag_name = run # good to name same as the run, will default to 'latest'\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate Docker images from cloned modules\n",
    "\n",
    "build_and_push_images(\\\n",
    "                      repo_directory = repo_directory, \\\n",
    "                      modules_to_run = modules_to_run, \\\n",
    "                      docker_username = docker_username, \\\n",
    "                      push = push, \\\n",
    "                      custom_tag_name = custom_tag_name \\\n",
    "                     )\n",
    "                      \n",
    "# The output should look something like \n",
    "# sha256:6900c3d99325a4a7c8b282d4a7a62f2a0f3fc673f03f5ca3333c2746bf20d06a\n",
    "# docker.io/travissimmons/setfinder:latest\n",
    "\n",
    "# Then, generate the Python script that runs the command\n",
    "# Change Rebuild docker to TRUE if you made module and tag changes for a one-click run \n",
    "\n",
    "generate_run_scripts(\n",
    "    run=run,\n",
    "    modules_to_run=modules_to_run,\n",
    "    script_jobs=script_jobs,\n",
    "    base_dir=base_dir,\n",
    "    repo_directory=repo_directory,\n",
    "    rebuild_docker=False,\n",
    "    docker_username=docker_username,\n",
    "    push=False,\n",
    "    custom_tag_name=run,\n",
    "    default_jobs_json_path=os.path.join(base_dir, f\"confluence_{run}\", f\"{run}_mnt\", \"input\", \"reaches_of_interest.json\")\n",
    ")\n",
    "\n",
    "generate_run_all_modules_bash(\n",
    "    modules_to_run=modules_to_run,\n",
    "    script_jobs=script_jobs,\n",
    "    base_dir=base_dir,\n",
    "    script_folder_name = \"sh_scripts\",\n",
    "    bash_script_name = \"run_all_modules.sh\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running this notebook, there will be individual python scripts and run_all_module.sh file generated in the same directory.\n",
    "# You can either run the python scripts individually or run the .sh script to run them all on the command line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
