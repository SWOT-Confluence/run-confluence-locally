{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Functions (IGNORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS IGNORE\n",
    "def build_and_push_images(repo_directory:str, target_repo_names:list, docker_username:str, push:bool = True, custom_tag_name:str = 'latest'):\n",
    "    for a_repo_name in target_repo_names:\n",
    "        repo_path = os.path.join(repo_directory, a_repo_name)\n",
    "        docker_path = f'{docker_username}/{a_repo_name}:{custom_tag_name}'\n",
    "        build_cmd = ['docker', 'build','--quiet', '-f', os.path.join(repo_path, \"Dockerfile\"), '-t', docker_path, repo_path]\n",
    "        try:\n",
    "            sp.run(build_cmd)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Docker build failed...\\n\"\n",
    "                f\"Build Command: {build_cmd}\\n\"\n",
    "                f\"Error: {e}\"\n",
    "            )\n",
    "        if push:\n",
    "            try:\n",
    "                push_cmd = ['docker', 'push', docker_path]\n",
    "                sp.run(push_cmd)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Docker push failed...\\n\"\n",
    "                    f\"Push Command: {push_cmd}\\n\"\n",
    "                    f\"Error: {e}\"\n",
    "                )\n",
    "    \n",
    "def build_sifs_and_create_slurm_scripts(run_list, included_modules, base_dir, docker_username, build):\n",
    "\n",
    "    for run in run_list:\n",
    "        \n",
    "        # Has to exist with 'mnt' structure (Doit exister avec la structure 'mnt')\n",
    "        mnt_dir = os.path.join(base_dir, f'confluence_{run}', f'{run}_mnt')\n",
    "        \n",
    "        # Create the sh_scripts directory (Cree le repertoire sh_scripts)\n",
    "        sh_dir = os.path.join(base_dir, f'confluence_{run}', 'sh_scripts')\n",
    "        if not os.path.exists(sh_dir):\n",
    "            os.makedirs(sh_dir)\n",
    "        \n",
    "        # Create the sif directory (Cree la repertoire sif)\n",
    "        sif_dir = os.path.join(base_dir, f'confluence_{run}', 'sif')\n",
    "        if not os.path.exists(sif_dir):\n",
    "            os.makedirs(sif_dir)\n",
    "        \n",
    "        # Create the report directory (Cree la repertoire report)\n",
    "        report_dir = os.path.join(base_dir, f'confluence_{run}', 'report')\n",
    "        if not os.path.exists(report_dir):\n",
    "            os.makedirs(report_dir)\n",
    "\n",
    "\n",
    "        submission_prefix = '#SBATCH'\n",
    "\n",
    "\n",
    "        job_details = {\n",
    "        'partition': 'cpu-preempt',\n",
    "        'nodes' : '1',\n",
    "        'cpus-per-task': '1',\n",
    "        'job-name': f'{run}_cfl',\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "        command_dict = {\n",
    "            'expanded_setfinder': 'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'setfinder.simg') + ' -r reaches_of_interest.json -c continent.json -e -s 16 -o /data -n /data -a MetroMan HiVDI SIC NeoBAM -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'expanded_combine_data': 'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'combine_data.simg') + ' -d /data  -e -s 16',\n",
    "            'input': 'singularity run --bind ' + f'{mnt_dir}/input:/mnt/data ' + os.path.join(sif_dir, 'input.simg') + ' -r /mnt/data/expanded_reaches_of_interest.json -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'non_expanded_setfinder': 'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'setfinder.simg') + ' -c continent.json -s 16 -o /data -n /data -a MetroMan HiVDI SIC NeoBAM -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'non_expanded_combine_data': 'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'combine_data.simg') + ' -d /data -s 16',\n",
    "            'prediagnostics': 'singularity run --bind ' + f'{mnt_dir}/input:/mnt/data/input,{mnt_dir}/diagnostics/prediagnostics:/mnt/data/output ' + os.path.join(sif_dir, f'prediagnostics.simg') + ' -i ${SLURM_ARRAY_TASK_ID} -r reaches.json',\n",
    "            'constrained_priors': f'singularity run -c --writable-tmpfs --bind {mnt_dir}/input:/mnt/data {os.path.join(sif_dir, \"priors.simg\")} ' + ' -i ${SLURM_ARRAY_TASK_ID} -r constrained -p usgs riggs -g -s local',\n",
    "            'unconstrained_priors': f'singularity run -c --writable-tmpfs --bind {mnt_dir}/input:/mnt/data {os.path.join(sif_dir, \"priors.simg\")} ' + ' -i ${SLURM_ARRAY_TASK_ID} -r unconstrained -p usgs riggs -g -s local',\n",
    "            'ml_priors': f'singularity run -c --writable-tmpfs --bind {mnt_dir}/input:/mnt/data {os.path.join(sif_dir, \"priors.simg\")} ' + ' -i ${SLURM_ARRAY_TASK_ID} -r ml -p usgs riggs -g -s local',\n",
    "            'hivdi': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/hivdi:/mnt/data/output ' + os.path.join(sif_dir, 'hivdi.simg') + ' /mnt/data/input/reaches.json --input-dir /mnt/data/input -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'sic4dvar': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/sic4dvar:/mnt/data/output,{mnt_dir}/logs:/mnt/data/logs '+ os.path.join(sif_dir, 'sic4dvar.simg') + ' -r reaches.json --index ${SLURM_ARRAY_TASK_ID}',\n",
    "            'metroman': f'singularity run --env AWS_BATCH_JOB_ID=\"foo\" --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/metroman:/mnt/data/output ' + os.path.join(sif_dir, \"metroman.simg\") + ' -i ${SLURM_ARRAY_TASK_ID} -r metrosets.json -s local -v',\n",
    "            'metroman_consolidation': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/metroman:/mnt/data/flpe ' + os.path.join(sif_dir, 'metroman_consolidation.simg') + ' -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'unconstrained_momma': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/momma:/mnt/data/output ' + os.path.join(sif_dir, 'momma.simg') + ' -r reaches.json -m 3 -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'neobam': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/geobam:/mnt/data/output ' + os.path.join(sif_dir, 'neobam.simg') + ' -r reaches.json -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'sad': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/sad:/mnt/data/output ' + os.path.join(sif_dir, 'sad.simg') + ' --reachfile reaches.json --index ${SLURM_ARRAY_TASK_ID}',\n",
    "            'moi': f'singularity run --env AWS_BATCH_JOB_ID=\"foo\" --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/output ' + os.path.join(sif_dir, 'moi.simg') + ' -j basin.json -v -b unconstrained -i ${SLURM_ARRAY_TASK_ID}', # -s local\n",
    "            'unconstrained_offline': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/moi,{mnt_dir}/offline:/mnt/data/output ' + os.path.join(sif_dir, 'offline.simg') + ' unconstrained timeseries integrator reaches.json ${SLURM_ARRAY_TASK_ID}',\n",
    "            'validation': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/moi,{mnt_dir}/offline:/mnt/data/offline,{mnt_dir}/validation:/mnt/data/output ' + os.path.join(sif_dir, 'validation.simg') + ' -r reaches.json -t unconstrained -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'output': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/diagnostics:/mnt/data/diagnostics,{mnt_dir}/moi:/mnt/data/moi,{mnt_dir}/offline:/mnt/data/offline,{mnt_dir}/validation:/mnt/data/validation,{mnt_dir}/output:/mnt/data/output ' + os.path.join(sif_dir, 'output.simg') + ' -s local -j /app/metadata/metadata.json -m input priors prediagnostics momma hivdi neobam metroman sic4dvar sad validation swot -i ${SLURM_ARRAY_TASK_ID}'\n",
    "        }\n",
    "        \n",
    " \n",
    "\n",
    "        def create_slurm_script(job_details=job_details, build_image=False, sif_dir='foo'):\n",
    "            submission_prefix = job_details['submission_prefix']\n",
    "            if build_image:\n",
    "                module_name = job_details['module_name']\n",
    "                image_name = module_name.replace('expanded_', '').replace('non_', '').replace('unconstrained_', '').replace('constrained_', '')\n",
    "                sp.run(['singularity', 'build', '-F', os.path.join(sif_dir, image_name + '.simg'), f\"docker://{job_details['docker_username']}/{image_name}\"])\n",
    "\n",
    "            file = open(os.path.join(sh_dir, f'{module_to_run}.sh'), 'w')\n",
    "            file.write('#!/bin/bash \\n')\n",
    "            file.write(f'{submission_prefix} -o {os.path.join(report_dir, f\"{module_to_run}.%a.out\")}' + ' \\n')\n",
    "\n",
    "            for item in job_details:\n",
    "                if item not in ['run_command', 'module_name', 'docker_username', 'submission_prefix']:\n",
    "                    file.write(f'{submission_prefix} --{item}={job_details[item]} \\n')\n",
    "            \n",
    "            file.write('\\n')            \n",
    "\n",
    "            # Insert GLOBAL_INDEX snippet for all except excluded modules\n",
    "            excluded = ['expanded_setfinder', 'expanded_combine_data', 'non_expanded_setfinder', 'non_expanded_combine_data', 'output']\n",
    "            if module_name not in excluded:\n",
    "                file.write('GLOBAL_INDEX=$(( ${OFFSET:-0} + SLURM_ARRAY_TASK_ID ))\\n')\n",
    "                file.write('echo \"OFFSET = ${OFFSET:-0}\"\\n')\n",
    "                file.write('echo \"SLURM_ARRAY_TASK_ID = ${SLURM_ARRAY_TASK_ID}\"\\n')\n",
    "                file.write('echo \"GLOBAL_INDEX = ${GLOBAL_INDEX}\"\\n\\n')\n",
    "            \n",
    "            file.write('\\n')            \n",
    "            file.write(job_details[\"run_command\"])\n",
    "            file.write('\\n')\n",
    "            \n",
    "            file.close()\n",
    "\n",
    "        included_modules = included_modules\n",
    "\n",
    "        for module_to_run, run_command in command_dict.items():\n",
    "            \n",
    "            if module_to_run == 'moi':\n",
    "                time_to_use = '00:30:00'\n",
    "                mem_to_use = '2G'\n",
    "            elif module_to_run == 'neobam':\n",
    "                time_to_use = '02:00:00'\n",
    "                mem_to_use = '48G'\n",
    "            elif module_to_run == 'output':\n",
    "                time_to_use = '05:00:00'\n",
    "                mem_to_use = '2G'\n",
    "            else:\n",
    "                time_to_use = '02:00:00'\n",
    "                mem_to_use = '4G'\n",
    "                \n",
    "            if included_modules:\n",
    "                if module_to_run not in included_modules:\n",
    "                    continue\n",
    "\n",
    "            print('DIRECTORY NAME: ', run, '\\nMODULE: ', module_to_run)\n",
    "            \n",
    "\n",
    "\n",
    "            job_details.update({\n",
    "                'run_command': run_command,\n",
    "                'module_name': module_to_run,\n",
    "                'mem': mem_to_use,\n",
    "                'time': time_to_use,\n",
    "                'docker_username': docker_username,\n",
    "                'submission_prefix': submission_prefix,\n",
    "                'job-name': f'{module_to_run}_{run}_cfl',\n",
    "\n",
    "            })\n",
    "            create_slurm_script(job_details=job_details, build_image=build, sif_dir=sif_dir)\n",
    "\n",
    "                \n",
    "def generate_slurm_driver(\n",
    "    job_name: str,\n",
    "    output_log_dir: str,\n",
    "    partition: str,\n",
    "    time_limit: str,\n",
    "    nodes: int,\n",
    "    ntasks: int,\n",
    "    cpus_per_task: int,\n",
    "    mem: str,\n",
    "    run: str,\n",
    "    directory: str,\n",
    "    json_file: str,\n",
    "    batch_size: int,\n",
    "    concurrent_jobs: int,\n",
    "    script_jobs: dict[str, str],\n",
    "    scripts: list[str]\n",
    ") -> str:\n",
    "    slurm_header = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --output={output_log_dir}/{job_name}_%j.out\n",
    "#SBATCH --error={output_log_dir}/{job_name}_%j.err\n",
    "#SBATCH --partition={partition}\n",
    "#SBATCH --time={time_limit}\n",
    "#SBATCH --nodes={nodes}\n",
    "#SBATCH --ntasks={ntasks}\n",
    "#SBATCH --cpus-per-task={cpus_per_task}\n",
    "#SBATCH --mem={mem}\n",
    "\n",
    "run='{run}'\n",
    "echo \"Run: $run\"\n",
    "\n",
    "directory=\"{directory}\"\n",
    "\n",
    "# Parameters\n",
    "json_file=\"{json_file}\"\n",
    "expanded_json_file=\"{expanded_json_file}\"\n",
    "default_jobs=$(jq length \"$json_file\")\n",
    "\n",
    "# Adjust to HPC requirements\n",
    "batch_size={batch_size}\n",
    "concurrent_jobs={concurrent_jobs}\n",
    "\n",
    "# Map specific script names to their job counts\n",
    "declare -A script_jobs=(\n",
    "\"\"\"\n",
    "\n",
    "    # Inject job counts into script_jobs associative array\n",
    "    for script, jobs in script_jobs.items():\n",
    "        slurm_header += f\"    [{script}]={jobs}\\n\"\n",
    "    slurm_header += \")\\n\\n\"\n",
    "\n",
    "    # Build scripts array\n",
    "    script_array = '    ' + '\\n    '.join(scripts)\n",
    "    scripts_block = f\"\"\"scripts=(\n",
    "{script_array}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "    body = f\"\"\"{scripts_block}\n",
    "\n",
    "\n",
    "# Change Job Numbers for expanded once it exists\n",
    "if [[ -s \"${{directory}}/${{input}}/${{expanded_reaches_of_interest.json}}\" ]]; then\n",
    "  expanded_jobs=$(jq length \"${{directory}}/${{input}}/${{expanded_reaches_of_interest.json}}\")\n",
    "  script_jobs[\"input.sh\"]=$expanded_jobs\n",
    "fi\n",
    "\n",
    "# Change Job Numbers for all once non_expanded_combine_data is run\n",
    "if [[ -s \"${{directory}}/${{input}}/${{expanded.json}}\" ]]; then\n",
    "  reaches_jobs=$(jq length \"${{directory}}/${{input}}/${{expanded.json}}\")\n",
    "  metrosets_jobs=$(jq length \"${{directory}}/${{input}}/${{metrosets.json}}\")\n",
    "  basin_jobs=$(jq length \"${{directory}}/${{input}}/${{basin.json}}\")\n",
    "  script_jobs[\"prediagnostics.sh\"]=$reaches_jobs\n",
    "  script_jobs[\"metroman.sh\"]=$metrosets_jobs\n",
    "  script_jobs[\"metroman_consolidation.sh\"]=$reaches_jobs\n",
    "  script_jobs[\"sic4dvar.sh\"]=$reaches_jobs\n",
    "  script_jobs[\"unconstrained_momma.sh\"]=$reaches_jobs\n",
    "  script_jobs[\"neobam.sh\"]=$reaches_jobs\n",
    "  script_jobs[\"moi.sh\"]=$basin_jobs\n",
    "  script_jobs[\"unconstrained_offline.sh\"]=$reaches_jobs\n",
    "  script_jobs[\"validation.sh\"]=$reaches_jobs\n",
    "fi\n",
    "\n",
    "\n",
    "for slurm_script in \"${{scripts[@]}}\"; do\n",
    "    echo \"Starting submission for: $slurm_script\"\n",
    "    date\n",
    "\n",
    "    num_jobs=\"${{script_jobs[$slurm_script]}}\"\n",
    "    if [[ -z \"$num_jobs\" ]]; then\n",
    "        echo \"Warning: No job count found for $slurm_script. Skipping.\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    start=0\n",
    "    while [ $start -lt $num_jobs ]; do\n",
    "        end=$((start + batch_size - 1))\n",
    "        if [ $end -ge $num_jobs ]; then\n",
    "            end=$((num_jobs - 1))\n",
    "        fi\n",
    "\n",
    "        echo \"Submitting jobs $start to $end from $slurm_script\"\n",
    "        # job_id=$(sbatch --array=${{start}}-${{end}}%${{concurrent_jobs}} \"${{directory}}/${{slurm_script}}\")\n",
    "        job_id=$(\n",
    "            sbatch --export=ALL,OFFSET=${start} \\\n",
    "           --array=0-$((end - start))%${concurrent_jobs} \\\n",
    "           \"${directory}/${slurm_script}\"\n",
    "        )\n",
    "        job_id_number=$(echo $job_id | awk '{{print $4}}')\n",
    "\n",
    "        echo \"Waiting for job array $job_id_number to finish...\"\n",
    "        while squeue -j \"$job_id_number\" 2>/dev/null | grep -q \"$job_id_number\"; do\n",
    "            job_info=$(squeue -j \"${{job_id_number}}[]\" --noheader -o \"%i %T %R\")\n",
    "            held_tasks=$(echo \"$job_info\" | grep -i \"launch failed requeued held\" | awk '{{print $1}}')\n",
    "\n",
    "            if [[ -n \"$held_tasks\" ]]; then\n",
    "                echo \"Detected held tasks in array $job_id_number:\"\n",
    "                echo \"$held_tasks\"\n",
    "                for task in $held_tasks; do\n",
    "                    echo \"Cancelling task $task...\"\n",
    "                    scancel \"$task\"\n",
    "                done\n",
    "            fi\n",
    "\n",
    "            sleep 10\n",
    "        done\n",
    "\n",
    "        echo \"Batch $job_id_number has finished. Submitting next batch.\"\n",
    "        date\n",
    "\n",
    "        start=$((end + 1))\n",
    "        sleep 5\n",
    "    done      \n",
    "    \n",
    "done\n",
    "\n",
    "echo \"Run $run has finished successfully.\"\n",
    "\"\"\"\n",
    "    return slurm_header + body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Prepare Docker Images (RUN LOCALLY, NOT HPC)\n",
    "* Builds docker images locally and stores them on your dockerhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------\n",
    "\n",
    "# SETUP\n",
    "\n",
    "# Directory where you are storing repos\n",
    "repo_directory = '/nas/cee-water/cjgleason/ellie/SWOT/confluence/modules/'\n",
    "target_repo_names = ['setfinder', 'combine_data']\n",
    "\n",
    "# Only provide this if you want to store images on dockerhub to move to HPC (you probably do)\n",
    "push = True\n",
    "docker_username = 'efrie130' # replace with your Docker Hub username\n",
    "custom_tag_name = 'hpc' # leave this as latest or rename to same name as run for version\n",
    "\n",
    "# --------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Docker build failed...\nBuild Command: ['docker', 'build', '--quiet', '-f', '/nas/cee-water/cjgleason/ellie/SWOT/confluence/modules/setfinder/Dockerfile', '-t', 'efrie130/setfinder:hpc', '/nas/cee-water/cjgleason/ellie/SWOT/confluence/modules/setfinder']\nError: [Errno 2] No such file or directory: 'docker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m, in \u001b[0;36mbuild_and_push_images\u001b[0;34m(repo_directory, target_repo_names, docker_username, push, custom_tag_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/modules/apps/ood/jupyterlab-matlab/lib/python3.11/subprocess.py:546\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 546\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/modules/apps/ood/jupyterlab-matlab/lib/python3.11/subprocess.py:1022\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1020\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1022\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/modules/apps/ood/jupyterlab-matlab/lib/python3.11/subprocess.py:1899\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1898\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1899\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1900\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'docker'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbuild_and_push_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mrepo_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrepo_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtarget_repo_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_repo_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdocker_username\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocker_username\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mpush\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpush\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcustom_tag_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcustom_tag_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# The output should look something like \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# sha256:6900c3d99325a4a7c8b282d4a7a62f2a0f3fc673f03f5ca3333c2746bf20d06a\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# docker.io/travissimmons/setfinder:latest\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m, in \u001b[0;36mbuild_and_push_images\u001b[0;34m(repo_directory, target_repo_names, docker_username, push, custom_tag_name)\u001b[0m\n\u001b[1;32m      8\u001b[0m     sp\u001b[38;5;241m.\u001b[39mrun(build_cmd)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocker build failed...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild Command: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbuild_cmd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m push:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Docker build failed...\nBuild Command: ['docker', 'build', '--quiet', '-f', '/nas/cee-water/cjgleason/ellie/SWOT/confluence/modules/setfinder/Dockerfile', '-t', 'efrie130/setfinder:hpc', '/nas/cee-water/cjgleason/ellie/SWOT/confluence/modules/setfinder']\nError: [Errno 2] No such file or directory: 'docker'"
     ]
    }
   ],
   "source": [
    "build_and_push_images(\\\n",
    "                      repo_directory = repo_directory, \\\n",
    "                      target_repo_names = target_repo_names, \\\n",
    "                      docker_username = docker_username, \\\n",
    "                      push = push, \\\n",
    "                      custom_tag_name = custom_tag_name \\\n",
    "                     )\n",
    "                      \n",
    "# The output should look something like \n",
    "# sha256:6900c3d99325a4a7c8b282d4a7a62f2a0f3fc673f03f5ca3333c2746bf20d06a\n",
    "# docker.io/travissimmons/setfinder:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Create Confluence Folder Structure (HPC OR LOCAL DEPENDING ON YOUR NOTEBOOK)\n",
    "* Copies empty directory structure which Confluence scripts point to and hosts output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "# !conda install -c conda-forge gdown -y\n",
    "!gdown 16FdIV0xyaQaNfvxR7OJ_p8ljaI9gv1pu\n",
    "!tar -xzvf confluence_empty.tar.gz\n",
    "!mv ./confluence_empty ./confluence_xxx\n",
    "!mv ./confluence_xxx/empty_mnt ./confluence_xxx/empty_xxx\n",
    "#rename confluence parent folder suffix and _mnt prefix to same as tag/run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Confluence Module SLURM Script Generator (RUN ON HPC, NOT LOCALLY)\n",
    "* Builds sif files from your dockerhub and generates scripts to submit to a SLURM job scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "\n",
    "# SETUP\n",
    "\n",
    "# Directory where you are storing repos\n",
    "base_dir = '/Users/.../confluence/'\n",
    "included_modules= {'expanded_combine_data', 'expanded_setfinder', 'input'}\n",
    "docker_username = 'efrie130'\n",
    "custom_tag_name = 'latest' # leave this as latest unless you have a really good reason!\n",
    "\n",
    "base_dir = '/Users/efriedmann/OneDrive - University of Massachusetts/UMass/SWOT/confluence/confluence_test/'\n",
    "included_modules= {'expanded_combine_data', 'expanded_setfinder', 'input','prediagnostics'}\n",
    "docker_username = 'efrie130'\n",
    "custom_tag_name = 'latest' # leave this as latest unless you have a really good reason!\n",
    "# Providing a run list will create slum scripts to run \n",
    "run_list = ['test'] # SAME AS FOLDER STRUCTURE xxx, can be multiple directories if setting up mutliple runs\n",
    "\n",
    "# Rebuild the sif\n",
    "build = True\n",
    "\n",
    "# --------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORY NAME:  test \n",
      "MODULE:  expanded_setfinder\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbuild_sifs_and_create_slurm_scripts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mincluded_modules\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mincluded_modules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdocker_username\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocker_username\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuild\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 137\u001b[0m, in \u001b[0;36mbuild_sifs_and_create_slurm_scripts\u001b[1;34m(run_list, included_modules, base_dir, docker_username, build)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDIRECTORY NAME: \u001b[39m\u001b[38;5;124m'\u001b[39m, run, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMODULE: \u001b[39m\u001b[38;5;124m'\u001b[39m, module_to_run)\n\u001b[0;32m    127\u001b[0m job_details\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_command\u001b[39m\u001b[38;5;124m'\u001b[39m: run_command,\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule_name\u001b[39m\u001b[38;5;124m'\u001b[39m: module_to_run,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \n\u001b[0;32m    136\u001b[0m })\n\u001b[1;32m--> 137\u001b[0m \u001b[43mcreate_slurm_script\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_details\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuild_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msif_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msif_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 90\u001b[0m, in \u001b[0;36mbuild_sifs_and_create_slurm_scripts.<locals>.create_slurm_script\u001b[1;34m(job_details, build_image, sif_dir)\u001b[0m\n\u001b[0;32m     88\u001b[0m     module_name \u001b[38;5;241m=\u001b[39m job_details[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     89\u001b[0m     image_name \u001b[38;5;241m=\u001b[39m module_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpanded_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munconstrained_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstrained_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m     \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msingularity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbuild\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-F\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msif_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.simg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocker://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mjob_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocker_username\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimage_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sh_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_to_run\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.sh\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     93\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#!/bin/bash \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\efriedmann\\Anaconda3\\envs\\gee2\\Lib\\subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\efriedmann\\Anaconda3\\envs\\gee2\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\efriedmann\\Anaconda3\\envs\\gee2\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1540\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "build_sifs_and_create_slurm_scripts(run_list=run_list, \\\n",
    "                                    included_modules = included_modules, \\\n",
    "                                    base_dir = base_dir, \\\n",
    "                                    docker_username = docker_username,\n",
    "                                    build = build\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Confluence Driver Script Generator (RUN ON HPC, NOT LOCALLY)\n",
    "* Creates a batch submission script that will run all of your sif files in serial\n",
    "* use sbatch to submit the entire run\n",
    "* low resources and a long time should be used here, as all this job will do is launch your SLURM scripts you created for each module, it is basically a job manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create driver SLURM script for each run in run_list\n",
    "\n",
    "for run in run_list:\n",
    "\n",
    "    job_name = str(run)\n",
    "    output_log_dir = \"./log\"\n",
    "    partition = \"cpu-preempt\"\n",
    "    time_limit = \"30:00:00\"\n",
    "    ntasks = 1\n",
    "    cpus_per_task = 1\n",
    "    mem = \"5G\"\n",
    "\n",
    "    run = str(run)\n",
    "    directory = f\"/your_path_here/SWOT/confluence/confluence_{run}/sh_scripts\"\n",
    "    json_file = f\"{directory}{run}_mnt/input/reaches_of_interest.json\"\n",
    "    batch_size = 1000\n",
    "    concurrent_jobs = 400\n",
    "\n",
    "    # modify as needed for modules you need to run\n",
    "    script_jobs = {\n",
    "        \"expanded_setfinder.sh\": \"7\",\n",
    "        \"expanded_combine_data.sh\": \"1\",\n",
    "        \"input_so.sh\": \"$default_jobs\",\n",
    "        \"non_expanded_setfinder.sh\": \"7\",\n",
    "        \"non_expanded_combine_data.sh\": \"1\",\n",
    "        \"prediagnostics_permissive.sh\": \"$default_jobs\",\n",
    "        # \"unconstrained_priors.sh\": \"7\", \n",
    "        # \"constrained_priors.sh\": \"7\",\n",
    "        \"sad.sh\": \"$default_jobs\",\n",
    "        \"metroman.sh\": \"$default_jobs\",\n",
    "        \"metroman_consolidation.sh\": \"$default_jobs\",\n",
    "        \"sic4dvar.sh\": \"$default_jobs\",\n",
    "        \"unconstrained_momma.sh\": \"$default_jobs\",\n",
    "        \"neobam.sh\": \"$default_jobs\",\n",
    "        \"moi.sh\": \"$default_jobs\",\n",
    "        \"unconstrained_offline.sh\": \"$default_jobs\",\n",
    "        \"validation.sh\": \"$default_jobs\",\n",
    "        \"output.sh\": \"7\",\n",
    "    }\n",
    "\n",
    "    scripts = [\n",
    "        \"expanded_setfinder.sh\",\n",
    "        \"expanded_combine_data.sh\",\n",
    "        \"input_so.sh\",\n",
    "        \"non_expanded_setfinder.sh\",\n",
    "        \"non_expanded_combine_data.sh\",\n",
    "        \"prediagnostics_permissive.sh\",\n",
    "        # \"unconstrained_priors.sh\",\n",
    "        # \"constrained_priors.sh\",\n",
    "        \"sad.sh\",\n",
    "        \"metroman.sh\",\n",
    "        \"metroman_consolidation.sh\",\n",
    "        \"sic4dvar.sh\",\n",
    "        \"unconstrained_momma.sh\",\n",
    "        \"neobam.sh\",\n",
    "        \"moi.sh\",\n",
    "        \"unconstrained_offline.sh\",\n",
    "        \"validation.sh\",\n",
    "        \"output.sh\",\n",
    "    ]\n",
    "\n",
    "    driver_script = generate_slurm_driver(\n",
    "        job_name=job_name,\n",
    "        output_log_dir=output_log_dir,\n",
    "        partition=partition,\n",
    "        time_limit=time_limit,\n",
    "        ntasks=ntasks,\n",
    "        cpus_per_task=cpus_per_task,\n",
    "        mem=mem,\n",
    "        run=run,\n",
    "        directory=directory,\n",
    "        json_file=json_file,\n",
    "        batch_size=batch_size,\n",
    "        concurrent_jobs=concurrent_jobs,\n",
    "        script_jobs=script_jobs,\n",
    "        scripts=scripts,\n",
    "    )\n",
    "\n",
    "    # Save to file\n",
    "    with open(\"slurm_driver.sh\", \"w\") as f:\n",
    "        f.write(driver_script)\n",
    "\n",
    "\n",
    "# Optionally submit\n",
    "# import subprocess\n",
    "# subprocess.run([\"sbatch\", \"driver_submit.sh\"], check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Running Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to run on specific reaches\n",
    "* modify the file at /mnt/input/reaches_of_interest.json\n",
    "#### In order to change a module and test it\n",
    "* change the module locally, build it and push to dockerhub using the first part of this notebook and then run as usual\n",
    "* you can use the run_list variable to generate more submission script per moule to test more than one change at a time. However, whenver you submit them, they will still run one at a time, it just submits the next run automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
